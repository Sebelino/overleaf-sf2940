\documentclass{article}
\title{Problem set, SF2940}
\author{Ville Sebastian Olsson}
\usepackage[a4paper,margin=2em]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage[parfill]{parskip}
\usepackage{sebelino-mathlib} % Custom sty file
\setcounter{secnumdepth}{0}
\begin{document}
\maketitle

\section{Quiz 1.2}

Let \((\Omega, \mathcal{A}, P)\) be a probability space.
Let \(A\in \mathcal{A}\) and \(B\in \mathcal{A}\)
with \(A\subseteq B\).

\textbf{a)} Is this statement correct?:
\[P(B^\complement \cap A) = P(B)-P(A)\]

\textbf{Solution:}

No. Let \(A=\varnothing\) and \(P(B)>0\).

\begin{align*}
	 & LHS \\
	=& P(B^\complement \cap A) \\
	=& P(B^\complement \cap \varnothing) \\
	=& P(\varnothing) \\
	=& 0 \\
\end{align*}
while
\begin{align*}
	  & RHS \\
	 =& P(B)-P(A) \\
	 =& P(B)-P(\varnothing) \\
	 =& P(B)-0 \\
	 =& P(B) \\
	 >& 0
\end{align*}

\textbf{b)} Is this statement correct?:
\[P(B \cap A^\complement) = P(B)-P(A)\]

\textbf{Solution:}

Yes.
\begin{proof}
\begin{align*}
	 & P(B\cap A^\complement) \\
	=& P(B\setminus A) \\
	=& P(B)-P(A) & (\text{ since }A\subseteq B) \\
\end{align*}
\end{proof}

\section{Exercise 1.12.2.9}

Suppose \(A,B \in \mathcal{A}\).

Show that
\[\min(P(A),P(B)) \geq P(A\cap B) \geq P(A)+P(B)-1\]

\textbf{Solution:}

We first need to show that
\[\min(P(A), P(B)) \geq P(A \cap B)\]

\begin{proof}
Monotonicity theorem:
\[\forall A,B\in \mathcal{A}: (A\subseteq B \Rightarrow P(A)\leq P(B))\]

Since \(A\cap B \subseteq A\), \(P(A\cap B) \leq P(A)\).

Since \(A\cap B \subseteq B\), \(P(A\cap B) \leq P(B)\).

\begin{align*}
	 & P(A\cap B)\leq P(A) \wedge P(A\cap B)\leq P(B) \\
	\Rightarrow & P(A \cap B) \leq \min(P(A), P(B))
\end{align*}
\end{proof}

Next, we show that
\[P(A \cap B) \geq P(A)+P(B)-1\]

\begin{proof}
\begin{align*}
	 & P(A \cap B) \\
	=& P(A)+P(B)-P(A \cup B) \\
	\geq& P(A)+P(B)-1 & (\text{ since } P(B)\leq 1) \\
\end{align*}
\end{proof}

\section{Exercise 1.12.3.10}

Suppose \(\mathcal{A}_1\), \(\mathcal{A}_2\) are \(\sigma\)-algebras
of subsets of \(\Omega\).

Show that \(\mathcal{A}_1 \cap \mathcal{A}_2\) is a \(\sigma\)-algebra
of subsets of \(\Omega\).

\textbf{Solution:}
\begin{proof}
\(\mathcal{A}_1\cap\mathcal{A}_2\) is a \(\sigma\)-algebra all of the following conditions hold:
\[\Omega \in \mathcal{A}_1\cap\mathcal{A}_2 \tag{I}\]
\[\forall A\in \mathcal{A}_1\cap \mathcal{A}_2: A^\complement \in \mathcal{A}_1\cap \mathcal{A}_2 \tag{II}\]
\[\forall (A_n)_{n=1}^\infty\subseteq \mathcal{A}_1\cap \mathcal{A}_2: \bigcup_{n=1}^\infty A_n \in \mathcal{A}_1\cap \mathcal{A}_2 \tag{III}\]

To show these, we make use of the general rule:
\[x\in A \wedge x\in B \Leftrightarrow x\in A\cap B\]

Property (I):
\begin{align*}
	& \mathcal{A}_1\ \sigma\text{-algebra} \wedge \mathcal{A}_2\ \sigma\text{-algebra} \\
	\Rightarrow& \Omega \in \mathcal{A}_1 \wedge \Omega \in \mathcal{A}_2 \\
	\Rightarrow& \Omega \in \mathcal{A}_1 \cap \mathcal{A}_2 \\
\end{align*}

Property (II):

Let \(A \in \mathcal{A}_1 \cap \mathcal{A}_2\).
\begin{align*}
    & A \in \mathcal{A}_1\cap \mathcal{A}_2 \\
    \Rightarrow & A \in \mathcal{A}_1\ \wedge\ A \in \mathcal{A}_2 \\
    \Rightarrow & A^\complement \in \mathcal{A}_1\ \wedge\ A^\complement \in \mathcal{A}_1 & (\text{ since }\mathcal{A}_1,\mathcal{A}_2\ \sigma\text{-algebras}) \\
    \Rightarrow & A^\complement \in \mathcal{A}_1\cap\mathcal{A}_2 \\
\end{align*}
\[\therefore \forall A\in \mathcal{A}_1\cap \mathcal{A}_2: A^\complement\in\mathcal{A}_1\cap \mathcal{A}_2\]

Property (III):

Let \((A_n)_{n=1}^\infty\subseteq \mathcal{A}_1\cap \mathcal{A}_2\).

\begin{align*}
    & (A_n)_{n=1}^\infty\subseteq \mathcal{A}_1\cap \mathcal{A}_2 \\
    & (A_n)_{n=1}^\infty\subseteq \mathcal{A}_1\ \wedge\ (A_n)_{n=1}^\infty\subseteq \mathcal{A}_2 \\
    & \bigcup_{n=1}^\infty A_n \in \mathcal{A}_1\ \wedge\ \bigcup_{n=1}^\infty A_n \in \mathcal{A}_2 \\
    & \bigcup_{n=1}^\infty A_n \in \mathcal{A}_1\cap\mathcal{A}_2 \\
\end{align*}
\[\therefore \forall (A_n)_{n=1}^\infty \subseteq \mathcal{A}_1\cap\mathcal{A}_2: \bigcup_{n=1}^\infty A_n\in \mathcal{A}_1\cap \mathcal{A_2}\]

Since all three properties hold, \(\mathcal{A}_1\cap\mathcal{A}_2\) is a \(\sigma\)-algebra.
\end{proof}

\section{Timo Example 1.5.1, p.26 [draft]}

Let \((\Omega,\mathcal{A},P)\) be a probability space.
Let \(A\in \mathcal{A}\). Consider the indicator function of \(A\):
\[\mathds{1}_A=\casesiie{1}{\omega\in A}{0}{\omega\notin A}\]
Show that \(\mathds{1}_A\) is a random variable.

\textbf{Solution:}
\begin{proof}
\(\mathds{1}\) is a random variable if \(\mathds{1}\) is a measurable function.

\(\mathds{1}_A\) is a measurable function w.r.t. Borel space \((\mathbb{R}, \mathcal{B})\) if
\[\forall B \in \mathcal{B}: \mathds{1}_A^{-1}(B)\in \mathcal{A}\]
Let \(B \in \mathcal{B}\).
\begin{align*}
     & \mathds{1}^{-1}(B) \\
    =& \{\omega\in\Omega:\mathds{1}_A(\omega)\in B\} \\
    =& \{\omega\in\Omega:\casesiie{1}{\omega\in A}{0}{\omega\notin A}\in B\} \\
    =& \{\omega\in\Omega:(1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A)\} \\
    =& \begin{cases}A\text{ if }0\in B\wedge 1\in B\\B\\C\\D\end{cases} \\
\end{align*}
\end{proof}

\section{Exercise 2.23.4.7 (Markov's inequality) [draft]}

Suppose that \(X\) is a random variable
such that \(P(X\geq 0) = P(\{\omega : X(\omega)\geq 0\})=1\)

Show that
\[(\forall c>0): P(X\geq c) \leq \frac{E[X]}{c}\]

\textbf{Solution:}

Use indicator function

For \(A\in \mathcal{A}\) the indicator function of \(A\)
is the random variable \(\mathds{1}_A(\omega)=\casesii{1}{\omega\in A}{0}\)

\begin{proof}

Observe:

	\[E[\mathds{1}_A] = 0 \cdot P(\mathds{1}=0)+1\cdot P(\mathds{1}=1)\]
	\[ = P(\{\omega\in\Omega : \mathds{1}(\omega)=1\})\]
	\[ = P(A)\]

Let
\[E_C = \{\omega: X(\omega)\geq c\} = X^{-1}([c,\infty))\}\]
, measurable.

\[P(X\geq c) = P(E_C)=E[\mathds{1}_{E_C}]\]

\[E[X] \geq E[Y]\]

\[P(X\geq c) = \frac1c E[c\mathds{1}_{E_C}] = \frac1c E[X]]\]

\end{proof}

\section{Extra1 [draft]}

Let \(\mathcal{B}(\mathbb{R})\) be the Borel algebra on \(\mathbb{R}\)
defined by \(\mathcal{B}(\mathbb{R})=\sigma(\{(a,b): a<b \in \mathbb{R}\})\).

Show that \(\forall x \in \mathbb{R}:\{x\}\in \mathcal{B}(\mathbb{R})\).

\textbf{Solution:}

\(\{x\}\) is the countably infinite intersection of all intervals
\[\{(x,b):x<b\}\]

\[\cap_{n=1} A_n \]

---

\(\{x\} \in \mathcal{B}\), so complement is too.

\((-\infty,x)\cup (x,\infty)\in \mathcal{B}\)

\((-\infty,x)\in \mathcal{B}\)

Since \(\forall n: (x,x+n)\in \mathcal{B}\),
\((x,\infty)=\cup_{n=1}^\infty(x,x+n)\in \mathcal{B}\).

Similiarly
\((-\infty,x)=\cup_{n=1}^\infty (x-n,x)\in\mathcal{B}\)

Hence
\[(-\infty,x)\cup (x,\infty)\in \mathcal{B}\]
\[\{x\}\in \mathcal{B}\]

Alternatively:

\[\{x\} = \cap_{n=1}^{\infty} (x-\frac{1}{n}, x+\frac{1}{n})\]
\[= (\cup_{n=1}^{\infty} (x-\frac{1}{n}, x+\frac{1}{n})^c)^c\]

\section{Quiz 1.3}

Let \((\Omega, \mathcal{A}, P)\) be a probability space.
Let \(A\in \mathcal{A}\) and \(B\in \mathcal{A}\).

Let \(P((A\cup B)^\complement)=0.5\).

Let \(P(A\cap B)=0.2\).

What is the probability that either A or B but not both will occur?

\textbf{Solution:}

\begin{align*}
	 & P((A\cup B)\setminus(A\cap B)) \\
	=& P(A\cup B)-P(A\cap B) & (\text{ since }A\cap B \subseteq A\cup B) \\
	=& P(A\cup B)-0.2 \\
	=& 1-P((A \cup B)^\complement)-0.2 \\
	=& 1-0.5-0.2 \\
	=& 0.3 \\
\end{align*}

\section{Quiz 1.5}

Assume
\(P(A) \geq 1-\delta\) and \(P(B) \geq 1-\delta\) for some small \(\delta\geq0\).

Show that \(P(A\cap B) \geq 1-2\delta\).

\textbf{Solution:}

\begin{align*}
	 & P(A\cup B) \\
	=& P(A)+P(B)-P(A\cap B) \\
	\Rightarrow& \\
	 & P(A\cap B) \\
	=& P(A)+P(B)-P(A\cup B) \\
	\geq& (1-\delta)+P(B)-P(A\cup B) & (\text{since }P(A) \geq 1-\delta) \\
	\geq& (1-\delta)+(1-\delta)-P(A\cup B) & (\text{since }P(B) \geq 1-\delta) \\
	\geq& (1-\delta)+(1-\delta)-1 & (\text{since }P(A\cup B) \leq 1) \\
	=& 1-2\delta
\end{align*}

\section{Quiz 1.6}

Let \(X\) be a random variable with continuous distribution whose CDF is given by
\[F_X(x) = \begin{cases}
	0 &\text{ if } x\leq 0 \\
	cx^2 &\text{ if } 0<x<2 \\
	1 &\text{ if } x \geq 2 \\
\end{cases}\]

\textbf{a)} Determine the constant \(c\).

\textbf{Solution:}

\(F_X(2)=1\). Since \(X\) is a continuous r.v., \(F_X\) is continuous. So:

\begin{align*}
	 & \lim_{x\uparrow 2}F_X(x)=1 \\
	\Rightarrow & c\cdot 2^2=1 \\
	\Rightarrow & c=\frac14 \\
\end{align*}

\textbf{b)} Compute \(Var(X)\).

\textbf{Solution:}

\begin{align*}
	 & f_X(x) \\
	=& \frac{d}{dx}F_X(x) \\
	=& \frac{d}{dx}\casesiii{0}{x\leq 0}{cx^2}{0<x<2}{1} \\
	=& \casesiii{0}{x\leq 0}{2cx}{0<x<2}{0} \\
	=& \casesiii{0}{x\leq 0}{\frac12x}{0<x<2}{0} \\
	=& \casesii{\frac12x}{0<x<2}{0} \\
\end{align*}
\begin{align*}
	 & E[X] \\
	=& \int_{-\infty}^\infty x f_X(x)dx \\
	=& \int_{-\infty}^\infty x \casesii{\frac12x}{0<x<2}{0}dx \\
	=& \int_{0}^2 x \frac12x dx \\
	=& \int_{0}^2 \frac12x^2 dx \\
	=& \frac16\hakparen{x^3}_0^2 dx \\
	=& \frac16(2^3-0^3) \\
	=& \frac{4}{3} \\
\end{align*}
\begin{align*}
	 & E[X^2] \\
	=& \int_{-\infty}^\infty x^2 f_X(x)dx \\
	=& \int_{-\infty}^\infty x^2 \casesii{\frac12x}{0<x<2}{0}dx \\
	=& \int_{0}^2 x^2 \frac12x dx \\
	=& \int_{0}^2 \frac12x^3 dx \\
	=& \frac18\hakparen{x^4}_0^2 \\
	=& \frac18(2^4-0^4) \\
	=& 2 \\
\end{align*}
\begin{align*}
	 & Var(X) \\
	=& E[X^2]-E[X]^2 \\
	=& 2-\left(\frac{4}{3}\right)^2 \\
	=& \frac{18}{9}-\frac{16}{9} \\
	=& \frac{2}{9} \\
\end{align*}

\section{Quiz 1.7 [draft]}

Consider the Monty Hall problem.

Prove that the probability to win the car if you switch doors is 2/3.

\textbf{Solution:}

Let \(C_k\) be the event that the car lies behind door \(k\), where \(k\in \{1,2,3\}\).
Then
\[\forall k: P(C_k)=\frac13\]

Let \(O_k\) be the event that the host opens door \(k\), where \(k\in \{1,2,3\}\).

The host will always open a door with a goat behind it, and never a door with
the car behind it. So:
\[\forall k: P(O_k|C_k) = 0\]
\[\forall k: P(O_{k+1}|C_k) = \frac12\]
\[\forall k: P(O_{k+2}|C_k) = \frac12\]
where \(k+1\) and \(k+2\) are modulo 3.

\section{Misc problem 1 [draft]}

Prove \(P(a < X \leq b) = F_X(b)-F_X(b)\).

\textbf{Solution:}

\section{Exercise 2.6.2.4 [draft]}

\(X=^dY\) if \(P(X\leq z)=P(Y\leq z)\).

Let \(X \sim \mathcal{N}(0, 1)\).

Show that \(X=^d-X\).

\textbf{Solution:}

\begin{align*}
	 & P(X\leq z) \\
	=& \frac{1}{\sqrt{2\pi}} \int_{-\infty}^z e^{-\frac{x^2}{2}}dx \\
	=& [y=-x, dy=-dx] \\
	=& \frac{1}{\sqrt{2\pi}} \int_{\infty}^{-z} e^{-\frac{(-y)^2}{2}}(-dy) \\
	=& P(-X\leq z) \\
\end{align*}

Hence, \(X\) symmetric distribution.

\section{Exercise 2.6.2.5 [draft]}

\(Z\sim \mathcal{N}(\mu, \sigma^2)\)
and \(X=e^Z\).

Show that the PDF of \(X\) is
\[f_X(x)=\frac{1}{x\sigma\sqrt{2\pi}}e^{\frac{(log x-\mu)^2}{2\sigma^2}}\]
for \(x>0\).

\textbf{Solution:}

Want to show \(P(X\leq z)=\int_{\infty}^z \frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{(logx-\mu)^2}{2\sigma^2}}\)

Notice that \(X\geq 0 \Rightarrow f_X(z)=0\) if \(z\leq 0\).

Let \(z>0\).

\(P(X\leq z)=P(e^Z\leq z)=(log\ bijective)=P(Z\leq log z)=\frac{1}{\sqrt{2\pi \sigma^2}}\int_{-\infty}^{log 2}e^{-\frac{(x-\mu^2)}{2\sigma^2}}dx\)

\[= [x=log y, dx=dy/y] = \frac{1}{\sqrt{2\pi\sigma^2}}\int_0^z e^{-\frac{(log y - \mu)^2}{2\sigma^2}} \frac{dy}{y}\]

\[=\frac{1}{\sqrt{2\pi\sigma^2}}\int_{-\infty}^z f_X(y)dy \Rightarrow f_X(y)=\frac{1}{\sqrt{2\pi \sigma^2}y}e^{-\frac{(log y-\mu)^2}{2\sigma^2}}\]
\(y>0\)

(Limit becomes \(-\infty\) because log(0) approaches negative infinity)

\section{Exercise 2.6.3.1 [draft]}

The continuous bivariate r.v. \((X,Y)\) has PDF
\[f_{X,Y}(x,y)=\casesii{1}{-y<x<y \wedge 0<y<1}{0}\]

Show that \(Cov(X,Y)=0\), but \(X\) and \(Y\) are not independent.

\textbf{Solution:}

\[Cov(X,Y) = E[XY]-E[X]E[Y]\]

\(X\) and \(Y\) independent iff
\[f_{X,Y}(x,y)=f_X(x)f_Y(y)\]
\[\Rightarrow E[XY]=\int_\mathbb{R}\int_\mathbb{R} xy f_{X,Y}(x,y)dxdy\]
\[= \int_\mathbb{R}xf_X(x)dx \int_\mathbb{R} yf_Y(y)dy = E[x]\cdot E[Y]\]
\[\Rightarrow Cov(X,Y)=0\]

Now, let's show cov is zero.

\[E[XY]=\int_\mathbb{R} \int_\mathbb{R} xy f_{X,Y}dxdy\]
\[=\int_0^1\int_{-y}^y xy1dxdy\]
\[=(symmetry, x is odd)\]
\[=\int_0^1 0=0\]

\(f_X(x)=0\) if \(|x|\geq 1\).

Let \(|x|<1\).

\[f_X(x)=\int_\mathbb{R} f_{X,Y}(x,y)dy\]

\[=\int_{|x|}^1 1dy = 1-|x|\]

\[E[X] = \int_\mathbb{R} x f_X(x)dx\]
\[= \int_0^1 x (1-|x|)dx\]
\[= (by symmetry, odd function)\]
\[= 0\]

\(x(1-|x|)\) is odd because it\'s a product of \(x\) which is odd and \(1-|x|\) is even.

Own note: The support is a triangle with corners at (0,0), (0,1), (1,1).

Now lets show that X,Y not independent.

Assume X,Y independent.

Then \(f_{X,Y}(x,y)=f_X(x)f_Y(y)\)

\[\Rightarrow f_Y(y)=\frac{f_{X,Y}(x,y)}{f_X(x)}\]

Assume \(|x|<1\) (so denominator doesnt become 0).

\[f_Y(y)=\frac{\mathds{1}_{|x|, 1}(y)}{1-|x|}\]

But tis expression is a function of \(x\), so contradiction.

And \(f_Y(y)=2y\), so it cannot be equal to the expression above.

Another way is to just compare the values of \(f_{X,Y}(x,y)\) vs \(f_X(x)f_Y(y)\).

\section{Exercise 2.6.3.17 [draft]}

Multivariate random variable \((X,Y)\) with joint PDF:
\[f_{X,Y}(x,y)=\casesii{\frac{2}{(1+x+y)^3}}{x>0\wedge y>0}{0}\]

Show that

a)
\[f_{X+Y}(u)=\frac{2u}{(1+u)^3}, u>0\]

b)
\[f_{X-Y}(v)=\frac{1}{2(1+v)^2},-\infty<v<\infty\]

\textbf{Solution:}

a)
We seek PDF of \(X+Y\).

\[P(X+Y\leq t)=P(X\leq t-Y)\]
\[=\int_\mathbb{R}\int_{-\infty}^{t-y} f_{X,Y}(x,y)dxdy\]

We know that \(f_{X,Y}(t)=\frac{d}{dt}P(X+Y\leq t)\)
where t is a continuity point of \(f_{X+Y}\)

\[\frac{d}{dt} P(X+Y\leq t)=\frac{d}{dt}\int_\mathbb{R}\int_{-\infty}^{t-y} f_{X,Y}(x,y)dxdy\]
\[=\int_\mathbb{R} \frac{d}{dt}\int_{\infty}^{t-y} f_{X,Y}(x,y)dxdy\]
\[=(Leibniz\ rule)\]
\[=\int_\mathbb{R} f_{X,Y}(t-y,y)dy\]

Similarly,
\[f_{X-Y}(u)=\int_\mathbb{R} f_{X,Y}(u+y,y)dy\]

a)

\[f_{X+Y}(u) = \int_\mathbb{R} f_{X,Y}(u-y,y)dy\]
\[=(u-y>0 \wedge y>0)\]
\[=\int_0^u \frac{2}{(1+u-y+y)^3}dy\]
\[=\int_0^u \frac{2}{(1+u)^3}dy\]
\[=\frac{2u}{(1+u)^3}\]

b)

Pick \(v\in \mathbb{R}\).

\[f_{X-Y}(v) = \int_\mathbb{R} f_{X,Y}(v+y,y)dy\]
\[=(v+y>0 \wedge y>0)\]
\[\int_{\max(0,-v)}^\infty \frac{2}{(1+v+2y)^3}dy\]
\[=[-\frac{1}{2} \frac{1}{(1+v+2y)^2}]_{\max(0,-v)^\infty}\]
\[\frac{1}{2(1+v+2\max(0,-v))^2}\]
\[\frac{1}{2(1+|v|)^2}\]

\section{Gut, 3.1, p.24}

Show that if \(X \in C(0,1)\), then so is \(1/X\).

\textbf{Solution:}

\(X\) and \(1/X\) have the same distribution if \(f_X=f_{1/X}\):

\[f_X(x;x_0,\gamma) = \frac{1}{\pi\gamma(1+\frac{(x-x_0)^2}{\gamma^2})}\]
\[f_X(x;0,1) = \frac{1}{\pi \cdot 1 \cdot (1+\frac{(x-0)^2}{1^2})}\]
\[f_X(x;0,1) = \frac{1}{\pi (x^2+1)}\]
Let \(Y=g(X)\) where \(g: \mathbb{R}\setminus\{0\}\to\mathbb{R}\setminus\{0\}\), \(g(x) = \frac{1}{x}\).

\(g\) is bijective, strictly monotone, and its inverse is:
\[g^{-1}(x) = \frac{1}{x}\]

\begin{align*}
 & f_{1/X}(y;0,1) \\
=& f_Y(y;0,1) \\
=& f_X(g^{-1}(y);0,1) \cdot |\frac{d}{dy}g^{-1}(y)| \\
=& \frac{1}{\pi (g^{-1}(y)^2+1)} \cdot |\frac{d}{dy}\frac{1}{y}| \\
=& \frac{1}{\pi ((\frac{1}{y})^2+1)} \cdot |-\frac{1}{y^2}| \\
=& \frac{1}{\pi (\frac{1}{y^2}+1)} \cdot \frac{1}{y^2} \\
=& \frac{1}{\pi (1+y^2)} \\
=& f_X(y;0,1) \\
\end{align*}

\(\therefore f_{1/X}\in C(0,1)\)

\section{Gut, Exercise 1.1, p.57}

Let \(X_1,X_2 \sim U(0,1)\), iid.

Find the distribution of \(X_1+X_2\).

\textbf{Solution:}

Let \(Y=X_1+X_2\).

\begin{align*}
	 & f_Y(y) \\
	=& \int_{-\infty}^\infty f_{X_1,X_2}(x_1,y-x_1)dx_1 \\
	=& \int_{-\infty}^\infty f_{X_1}(x_1)f_{X_2}(y-x_1)dx_1 & (\text{ since } X_1,X_2\text{ iid.}) \\
	=& \int_{-\infty}^\infty \casesii{1}{0\leq x_1 \leq 1}{0}f_{X_2}(y-x_1)dx_1 \\
	=& \int_0^1 \casesii{1}{0\leq x_1 \leq 1}{0}f_{X_2}(y-x_1)dx_1 \\
	=& \int_0^1 1\cdot f_{X_2}(y-x_1)dx_1 \\
	=& \int_0^1 f_{X_2}(y-x_1)dx_1 \\
	=& \int_0^1 \casesii{1}{0\leq y-x_1\leq 1}{0}dx_1 \\
	=& \int_0^1 \casesii{1}{y-1\leq x_1\leq y}{0}dx_1 \\
	=& \int_0^1 \casesii{1}{y-1\leq x_1\leq y\wedge 0\leq x_1 \leq 1}{0}dx_1 \\
	=& \int_0^1 \casesii{1}{\max(0,y-1)\leq x_1\leq \min(1,y)}{0}dx_1 \\
	=& \casesii{\int_0^1 \casesii{1}{\max(0,y-1)\leq x_1\leq \min(1,y)}{0}dx_1}{y \leq 1}{\int_0^1 \casesii{1}{\max(0,y-1)\leq x_1\leq \min(1,y)}{0}dx_1} \\
	=& \casesii{\int_0^1 \casesii{1}{0\leq x_1\leq y}{0}dx_1}{y \leq 1}{\int_0^1 \casesii{1}{y-1\leq x_1\leq 1}{0}dx_1} \\
	=& \casesii{\int_0^y \casesii{1}{0\leq x_1\leq y}{0}dx_1}{y \leq 1}{\int_{y-1}^1 \casesii{1}{y-1\leq x_1\leq 1}{0}dx_1} \\
	=& \casesiii{\int_0^y 1dx_1}{0\leq x_1 \leq y \wedge y\leq 1}{\int_{y-1}^1 1dx_1}{y-1\leq x_1 \leq 1 \wedge \neg(y\leq 1)}{0} \\
	=& \casesiii{\int_0^y 1dx_1}{0\leq y \wedge y\leq 1}{\int_{y-1}^1 1dx_1}{y-1\leq 1 \wedge y>1}{0} \\
	=& \casesiii{\int_0^y 1dx_1}{0\leq y\leq 1}{\int_{y-1}^1 1dx_1}{1<y\leq 2}{0} \\
	=& \casesiii{\hakparen{x_1}_0^y}{0\leq y\leq 1}{\hakparen{x_1}_{y-1}^1}{1<y\leq 2}{0} \\
	=& \casesiii{y-0}{0\leq y\leq 1}{1-(y-1)}{1<y\leq 2}{0} \\
	=& \casesiii{y}{0\leq y\leq 1}{2-y}{1<y\leq 2}{0} \\
\end{align*}

\section{Modelltenta Problem 1}

Let \(f : \mathbb{R} \to \mathbb{R}\) be a Borel function, and \(X\) be a
random variable. Let \(Y = f(X)\).

Prove that \(Y\) is a random variable.

\textbf{Solution:}

\begin{proof}
Let \(X:\Omega\to \mathbb{R}\).
Let \(\mathcal{A}\) be a \(\sigma\)-algebra on \(\Omega\).
Let \(\mathcal{B}\) be the Borel algebra on \(\mathbb{R}\).

\(Y\) is a random variable if \(Y\) is a measurable function w.r.t. measurable
spaces \((\Omega, \mathcal{A})\), \((\mathbb{R}, \mathcal{B})\).

\(Y\) is such a measurable function if \(\forall B\in \mathcal{B}: Y^{-1}(B)\in \mathcal{A}\).

Let
\[B\in \mathcal{B}\tag{I}\]

We need to prove that \(Y^{-1}(B)\in \mathcal{A}\).

\begin{align*}
	 & Y^{-1}(B) \\
	=& \{\omega\in \Omega: Y(\omega)\in B\} \\
	=& \{\omega\in \Omega: f(X(\omega))\in B\} \\
	=& \{\omega\in \Omega: f^{-1}(f(X(\omega)))\in f^{-1}(B)\} \\
	=& \{\omega\in \Omega: X(\omega)\in f^{-1}(B)\} \\
	=& X(f^{-1}(B)) \tag{II} \\
\end{align*}

Since \(X\) is a random variable:
\[\forall B\in \mathcal{B}: X^{-1}(B)\in \mathcal{A} \tag{III}\]

The preimage of \(f:\mathbb{R}\to\mathbb{R}\) is \(f^{-1}: \mathcal{B} \to\mathcal{B}\), so:
\[\forall B \in \mathcal{B}: f^{-1}(B)\in \mathcal{B} \tag{IV}\]

(I) and (IV) imply:
\[f^{-1}(B) \in \mathcal{B} \tag{V}\]

(V) and (III) imply:
\[X^{-1}(f^{-1}(B))\in \mathcal{A}\tag{VI}\]

(VI) and (II) imply:
\[Y^{-1}(B) \in \mathcal{A}\]

So:
\[\forall B\in \mathcal{B}:Y^{-1}(B) \in \mathcal{A}\]

Therefore, \(Y\) is a measurable function.

Therefore, \(Y\) is a random variable.
\end{proof}


\end{document}
