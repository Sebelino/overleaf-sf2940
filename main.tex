\documentclass{article}
\title{Problem set, SF2940}
\author{Ville Sebastian Olsson}
\usepackage[a4paper,margin=2em]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage[parfill]{parskip}
\usepackage{hyperref} % Clickable ToC
\hypersetup{colorlinks}

\usepackage{sebelino-mathlib} % Custom sty file
\setcounter{secnumdepth}{0}
\begin{document}
\maketitle
\tableofcontents

\section{Quizzes}
\subsection{Quiz 1.2}

Let \((\Omega, \mathcal{A}, P)\) be a probability space.
Let \(A\in \mathcal{A}\) and \(B\in \mathcal{A}\)
with \(A\subseteq B\).

\textbf{a)} Is this statement correct?:
\[P(B^\complement \cap A) = P(B)-P(A)\]

\textbf{Solution:}

No. Let \(A=\varnothing\) and \(P(B)>0\).

\begin{align*}
	 & LHS \\
	=& P(B^\complement \cap A) \\
	=& P(B^\complement \cap \varnothing) \\
	=& P(\varnothing) \\
	=& 0 \\
\end{align*}
while
\begin{align*}
	  & RHS \\
	 =& P(B)-P(A) \\
	 =& P(B)-P(\varnothing) \\
	 =& P(B)-0 \\
	 =& P(B) \\
	 >& 0
\end{align*}

\textbf{b)} Is this statement correct?:
\[P(B \cap A^\complement) = P(B)-P(A)\]

\textbf{Solution:}

Yes.
\begin{proof}
\begin{align*}
	 & P(B\cap A^\complement) \\
	=& P(B\setminus A) \\
	=& P(B)-P(A) & (\text{ since }A\subseteq B) \\
\end{align*}
\end{proof}
\subsection{Quiz 1.3}

Let \((\Omega, \mathcal{A}, P)\) be a probability space.
Let \(A\in \mathcal{A}\) and \(B\in \mathcal{A}\).

Let \(P((A\cup B)^\complement)=0.5\).

Let \(P(A\cap B)=0.2\).

What is the probability that either A or B but not both will occur?

\textbf{Solution:}

\begin{align*}
	 & P((A\cup B)\setminus(A\cap B)) \\
	=& P(A\cup B)-P(A\cap B) & (\text{ since }A\cap B \subseteq A\cup B) \\
	=& P(A\cup B)-0.2 \\
	=& 1-P((A \cup B)^\complement)-0.2 \\
	=& 1-0.5-0.2 \\
	=& 0.3 \\
\end{align*}

\subsection{Quiz 1.5}

Assume
\(P(A) \geq 1-\delta\) and \(P(B) \geq 1-\delta\) for some small \(\delta\geq0\).

Show that \(P(A\cap B) \geq 1-2\delta\).

\textbf{Solution:}

\begin{align*}
	 & P(A\cup B) \\
	=& P(A)+P(B)-P(A\cap B) \\
	\Rightarrow& \\
	 & P(A\cap B) \\
	=& P(A)+P(B)-P(A\cup B) \\
	\geq& (1-\delta)+P(B)-P(A\cup B) & (\text{since }P(A) \geq 1-\delta) \\
	\geq& (1-\delta)+(1-\delta)-P(A\cup B) & (\text{since }P(B) \geq 1-\delta) \\
	\geq& (1-\delta)+(1-\delta)-1 & (\text{since }P(A\cup B) \leq 1) \\
	=& 1-2\delta
\end{align*}

\subsection{Quiz 1.6}

Let \(X\) be a random variable with continuous distribution whose CDF is given by
\[F_X(x) = \begin{cases}
	0 &\text{ if } x\leq 0 \\
	cx^2 &\text{ if } 0<x<2 \\
	1 &\text{ if } x \geq 2 \\
\end{cases}\]

\textbf{a)} Determine the constant \(c\).

\textbf{Solution:}

\(F_X(2)=1\). Since \(X\) is a continuous r.v., \(F_X\) is continuous. So:

\begin{align*}
	 & \lim_{x\uparrow 2}F_X(x)=1 \\
	\Rightarrow & c\cdot 2^2=1 \\
	\Rightarrow & c=\frac14 \\
\end{align*}

\textbf{b)} Compute \(Var(X)\).

\textbf{Solution:}

\begin{align*}
	 & f_X(x) \\
	=& \frac{d}{dx}F_X(x) \\
	=& \frac{d}{dx}\casesiii{0}{x\leq 0}{cx^2}{0<x<2}{1} \\
	=& \casesiii{0}{x\leq 0}{2cx}{0<x<2}{0} \\
	=& \casesiii{0}{x\leq 0}{\frac12x}{0<x<2}{0} \\
	=& \casesii{\frac12x}{0<x<2}{0} \\
\end{align*}
\begin{align*}
	 & E[X] \\
	=& \int_{-\infty}^\infty x f_X(x)dx \\
	=& \int_{-\infty}^\infty x \casesii{\frac12x}{0<x<2}{0}dx \\
	=& \int_{0}^2 x \frac12x dx \\
	=& \int_{0}^2 \frac12x^2 dx \\
	=& \frac16\hakparen{x^3}_0^2 dx \\
	=& \frac16(2^3-0^3) \\
	=& \frac{4}{3} \\
\end{align*}
\begin{align*}
	 & E[X^2] \\
	=& \int_{-\infty}^\infty x^2 f_X(x)dx \\
	=& \int_{-\infty}^\infty x^2 \casesii{\frac12x}{0<x<2}{0}dx \\
	=& \int_{0}^2 x^2 \frac12x dx \\
	=& \int_{0}^2 \frac12x^3 dx \\
	=& \frac18\hakparen{x^4}_0^2 \\
	=& \frac18(2^4-0^4) \\
	=& 2 \\
\end{align*}
\begin{align*}
	 & Var(X) \\
	=& E[X^2]-E[X]^2 \\
	=& 2-\left(\frac{4}{3}\right)^2 \\
	=& \frac{18}{9}-\frac{16}{9} \\
	=& \frac{2}{9} \\
\end{align*}

\subsection{Quiz 1.7}

Consider the Monty Hall problem.

Prove that the probability to win the car if you switch door is 2/3.

\textbf{Solution:}

\begin{proof}
The following events may occur, in chronological order:
\[C_k: \text{The car is placed behind door }k\]
\[S_k: \text{You select door }k\]
\[D_k: \text{You decide to open door }k\]
where \(k\in \{1,2,3\}\).

Equivalently, we want to prove that sticking to one's choice yields a probability of 1/3:
\[P(C_k|S_k \cap D_k)=\frac{1}{3}\]

The car was placed by any of the doors with equal probability, so:
\[\forall k: P(C_k)=\frac13\tag{I}\]

Since you do not know where the car is, the actual location of the car does not affect your decisions. So:
\[P(S_k\cap D_k|C_k)=P(S_k\cap D_k)\tag{II}\]
\begin{align*}
     & P(C_k|S_k\cap D_k) \\
    =& P(S_k\cap D_k|C_k)\frac{P(C_k)}{P(S_k\cap D_k)} & (\text{Bayes' theorem}) \\
    =& P(S_k\cap D_k|C_k)\frac{1/3}{P(S_k\cap D_k)} & (\text{apply (I)}) \\
    =& \frac{P(S_k\cap D_k|C_k)}{3P(S_k\cap D_k)} \\
    =& \frac{P(S_k\cap D_k)}{3P(S_k\cap D_k)} & (\text{apply (II)}) \\
    =& \frac{1}{3}
\end{align*}
Similarly, the probability of winning by switching doors is:
\begin{align*}
     & P(C_k^\complement|S_k\cap D_k^\complement) \\
    =& 1-P(C_k|S_k\cap D_k^\complement) \\
    =& 1-\frac{P(S_k\cap D_k^\complement|C_k)}{3P(S_k\cap D_k^\complement)} \\
    =& 1-\frac{1}{3} \\
    =& \frac{2}{3}
\end{align*}
\end{proof}

\section{Timo Koski problems}
\subsection{Example 1.5.1, p.26}

Let \((\Omega,\mathcal{A},P)\) be a probability space.
Let \(A\in \mathcal{A}\). Consider the indicator function of \(A\):
\[\mathds{1}_A(\omega)=\casesiie{1}{\omega\in A}{0}{\omega\notin A}\]
Show that \(\mathds{1}_A\) is a random variable.

\textbf{Solution:}
\begin{proof}
\(\mathds{1}\) is a random variable if \(\mathds{1}\) is a measurable function.

\(\mathds{1}_A\) is a measurable function w.r.t. Borel space \((\mathbb{R}, \mathcal{B})\) if
\[\forall B \in \mathcal{B}: \mathds{1}_A^{-1}(B)\in \mathcal{A}\]
Let \(B \in \mathcal{B}\).
\begin{align*}
     & \mathds{1}^{-1}_A(B) \\
    =& \{\omega\in\Omega:\mathds{1}_A(\omega)\in B\} \\
    =& \{\omega\in\Omega:\casesiie{1}{\omega\in A}{0}{\omega\notin A}\in B\} \\
    =& \{\omega\in\Omega:(1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A)\} \\
    =& \begin{cases}
    \{\omega\in\Omega:(1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A)\}&\text{ if }0\in B\wedge 1\in B\\
    \{\omega\in\Omega:(1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A)\}&\text{ if }0\in B\wedge 1\notin B\\
    \{\omega\in\Omega:(1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A)\}&\text{ if }0\notin B\wedge 1\in B\\
    \{\omega\in\Omega:(1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A)\}&\text{ if }0\notin B\wedge 1\notin B
    \end{cases} \\
    =& \begin{cases}
    \{\omega\in\Omega:((1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A))\wedge (0\in B\wedge 1\in B)\}&\text{ if }0\in B\wedge 1\in B\\
    \{\omega\in\Omega:((1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A))\wedge (0\in B\wedge 1\notin B)\}&\text{ if }0\in B\wedge 1\notin B\\
    \{\omega\in\Omega:((1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A))\wedge (0\notin B\wedge 1\in B)\}&\text{ if }0\notin B\wedge 1\in B\\
    \{\omega\in\Omega:((1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A))\wedge (0\notin B\wedge 1\notin B)\}&\text{ if }0\notin B\wedge 1\notin B
    \end{cases} \\
    =& \begin{cases}
    \{\omega\in\Omega:(0\in B \wedge 1\in B\wedge \omega\in A)\vee (0\in B\wedge 1\in B\wedge \omega \notin A)\}&\text{ if }0\in B\wedge 1\in B\\
    \{\omega\in\Omega:(0\in B \wedge 1\in B\wedge1\notin B\wedge \omega\in A)\vee (0\in B\wedge 1\notin B\wedge \omega \notin A)\}&\text{ if }0\in B\wedge 1\notin B\\
    \{\omega\in\Omega:(0\notin B\wedge 1\in B\wedge \omega\in A)\vee (0\notin B \wedge 0\in B\wedge \omega \notin A)\}&\text{ if }0\notin B\wedge 1\in B\\
    \{\omega\in\Omega:(0\notin B\wedge 1\notin B \wedge 1\in B\wedge \omega\in A)\vee (0\notin B\wedge 1\notin B \wedge 0\in B\wedge \omega \notin A)\}&\text{ if }0\notin B\wedge 1\notin B
    \end{cases} \\
    =& \begin{cases}
    \{\omega\in\Omega:(\omega\in A)\vee (\omega \notin A)\}&\text{ if }0\in B\wedge 1\in B\\
    \{\omega\in\Omega:\bot\vee (\omega \notin A)\}&\text{ if }0\in B\wedge 1\notin B\\
    \{\omega\in\Omega:(\omega\in A)\vee \bot\}&\text{ if }0\notin B\wedge 1\in B\\
    \{\omega\in\Omega:\bot\vee \bot\}&\text{ if }0\notin B\wedge 1\notin B
    \end{cases} \\
    =& \begin{cases}
    \{\omega\in\Omega:\omega\in A\cup A^\complement\}&\text{ if }0\in B\wedge 1\in B\\
    \{\omega\in\Omega:\omega \notin A\}&\text{ if }0\in B\wedge 1\notin B\\
    \{\omega\in\Omega:\omega\in A\}&\text{ if }0\notin B\wedge 1\in B\\
    \{\omega\in\Omega:\bot\}&\text{ if }0\notin B\wedge 1\notin B
    \end{cases} \\
    =& \begin{cases}
    \{\omega\in\Omega:\omega\in \Omega\}&\text{ if }0\in B\wedge 1\in B\\
    \{\omega\in\Omega:\omega \in A^\complement\}&\text{ if }0\in B\wedge 1\notin B\\
    \{\omega\in\Omega:\omega\in A\}&\text{ if }0\notin B\wedge 1\in B\\
    \{\omega\in\Omega:\bot\}&\text{ if }0\notin B\wedge 1\notin B
    \end{cases} \\
    =& \begin{cases}
    \Omega &\text{ if }0\in B\wedge 1\in B\\
    A^\complement&\text{ if }0\in B\wedge 1\notin B\\
    A&\text{ if }0\notin B\wedge 1\in B\\
    \varnothing&\text{ if }0\notin B\wedge 1\notin B
    \end{cases}
\end{align*}
Since \(\mathcal{A}\) is a \(\sigma\)-algebra and \(A\in \mathcal{A}\), \(\{\Omega, A^\complement, A, \varnothing\}\subseteq \mathcal{A}\).

So \(\mathrm{img}(\mathds{1}_A^{-1}) \subseteq \mathcal{A}\).

Since \(\mathrm{img}(\mathds{1}_A^{-1})\subseteq \mathcal{A}\), \(\mathds{1}_A^{-1}(B)\in \mathcal{A}\).
\[\therefore \forall B\in \mathcal{B}: \mathds{1}_A^{-1}(B)\in \mathcal{A}\]
So \(\mathds{1}_A\) is a measurable function.

So \(\mathds{1}_A\) is a random variable.
\end{proof}

\subsection{Exercise 1.12.2.9}

Suppose \(A,B \in \mathcal{A}\).

Show that
\[\min(P(A),P(B)) \geq P(A\cap B) \geq P(A)+P(B)-1\]

\textbf{Solution:}

We first need to show that
\[\min(P(A), P(B)) \geq P(A \cap B)\]

\begin{proof}
Monotonicity theorem:
\[\forall A,B\in \mathcal{A}: (A\subseteq B \Rightarrow P(A)\leq P(B))\]

Since \(A\cap B \subseteq A\), \(P(A\cap B) \leq P(A)\).

Since \(A\cap B \subseteq B\), \(P(A\cap B) \leq P(B)\).

\begin{align*}
	 & P(A\cap B)\leq P(A) \wedge P(A\cap B)\leq P(B) \\
	\Rightarrow & P(A \cap B) \leq \min(P(A), P(B))
\end{align*}
\end{proof}

Next, we show that
\[P(A \cap B) \geq P(A)+P(B)-1\]

\begin{proof}
\begin{align*}
	 & P(A \cap B) \\
	=& P(A)+P(B)-P(A \cup B) \\
	\geq& P(A)+P(B)-1 & (\text{ since } P(B)\leq 1) \\
\end{align*}
\end{proof}

\subsection{Exercise 1.12.3.6, p.45}

Suppose that \(X\) is a random variable
such that \(P(X\geq 0) = P(\{\omega : X(\omega)\geq 0\})=1\)

Show that
\[(\forall c>0): P(X\geq c) \leq \frac{E[X]}{c}\]

\textbf{Solution 1:}

\begin{proof}
Let \(c>0\).
\begin{align*}
     & E[X] \\
    =& \int_{-\infty}^\infty x f_X(x)dx \\
    =& \int_0^\infty x f_X(x)dx & (\text{since }\mathrm{supp}(f_X)=[0,\infty)) \\
    =& \int_0^c x f_X(x)dx+\int_c^\infty x f_X(x)dx \\
    \geq& \int_c^\infty x f_X(x)dx \\
    \geq& \int_c^\infty c f_X(x)dx & (\text{since }\forall x\in [c,\infty): x\geq c) \\
    =& c\int_c^\infty f_X(x)dx \\
    =& c\left(\int_{-\infty}^\infty f_X(x)dx-\int_{-\infty}^c f_X(x)dx\right) \\
    =& c(1-F_X(c)) \\
    =& c(1-P(X\leq c)) \\
    =& cP(X\geq c) \\
    \Rightarrow & E[X] \geq c P(X\geq c) \\
    \Rightarrow & P(X\geq c) \leq \frac{E[X]}{c}
\end{align*}
\[\therefore (\forall c>0):P(X\geq c) \leq \frac{E[X]}{c}\]
\end{proof}

\textbf{Solution 2:}

\begin{proof}
Let \(c>0\).

Let \(A\in\mathcal{A}\) be an event.

Let
\[\mathds{1}_A(\omega)=\casesiie{1}{\omega\in A}{0}{\omega \notin A}\]
Then:
\[\mathds{1}_{X\geq c}(\omega)=\casesiie{1}{\omega\in \{X\geq c\}}{0}{\omega\in\{X<c\}}\]
Or just:
\[\mathds{1}_{X\geq c}=\casesiie{1}{X\geq c}{0}{X<c}\]
Law of excluded middle implies:
\[X \geq c \vee \neg(X \geq c)\]
Assume \(X\geq c\).
\begin{align*}
    & X\geq c \\
    \Rightarrow& c\leq X \\
    \Rightarrow& c\cdot 1\leq X \\
    \Rightarrow& c\cdot \casesiie{1}{X\geq c}{0}{X<c}\leq X \\
    \Rightarrow& c\cdot \mathds{1}_{X\geq c}\leq X \\
\end{align*}
Assume \(\neg(X\geq c)\).
\begin{align*}
    & \neg(X\geq c) \\
    \Rightarrow& X<c \\
    \Rightarrow& 0<X<c & (\text{since }X>0) \\
    \Rightarrow& c\cdot 0<X<c \\
    \Rightarrow& c\cdot \casesiie{1}{X\geq c}{0}{X<c} \leq X \\
    \Rightarrow& c\cdot \mathds{1}_{X\geq c}\leq X \\
\end{align*}
\begin{align*}
    & (X\geq c \Rightarrow c\cdot \mathds{1}_{X\geq c}\leq X) \wedge (\neg(X\geq c) \Rightarrow c\cdot \mathds{1}_{X\geq c}\leq X) \\
    \Rightarrow& c\cdot \mathds{1}_{X\geq c}\leq X \\
    \Rightarrow& E[c\cdot \mathds{1}_{X\geq c}]\leq E[X] & (\text{since } E\text{ monotonically non-decreasing}) \\
    \Rightarrow& c\cdot E[\mathds{1}_{X\geq c}]\leq E[X] \\
    \Rightarrow& c\sum_{x\in \{0,1\}} x\cdot p_{\mathds{1}_{X\geq c}}(x)\leq E[X] \\
    \Rightarrow& c (0\cdot p_{\mathds{1}_{X\geq c}}(0)+1\cdot p_{\mathds{1}_{X\geq c}}(1))\leq E[X] \\
    \Rightarrow& c\cdot p_{\mathds{1}_{X\geq c}}(1)\leq E[X] \\
    \Rightarrow& c\cdot P(\mathds{1}_{X\geq c}=1)\leq E[X] \\
    \Rightarrow& c\cdot P\left(\casesiie{1}{X\geq c}{0}{X<c}=1\right)\leq E[X] \\
    \Rightarrow& c\cdot P(X\geq c)\leq E[X] \\
    \Rightarrow& P(X\geq c)\leq \frac{E[X]}{c} \\
\end{align*}
\[\therefore (\forall c>0):P(X\geq c)\leq \frac{E[X]}{c}\]
\end{proof}

\subsection{Exercise 1.12.3.10}

Suppose \(\mathcal{A}_1\), \(\mathcal{A}_2\) are \(\sigma\)-algebras
of subsets of \(\Omega\).

Show that \(\mathcal{A}_1 \cap \mathcal{A}_2\) is a \(\sigma\)-algebra
of subsets of \(\Omega\).

\textbf{Solution:}
\begin{proof}
\(\mathcal{A}_1\cap\mathcal{A}_2\) is a \(\sigma\)-algebra all of the following conditions hold:
\[\Omega \in \mathcal{A}_1\cap\mathcal{A}_2 \tag{I}\]
\[\forall A\in \mathcal{A}_1\cap \mathcal{A}_2: A^\complement \in \mathcal{A}_1\cap \mathcal{A}_2 \tag{II}\]
\[\forall (A_n)_{n=1}^\infty\subseteq \mathcal{A}_1\cap \mathcal{A}_2: \bigcup_{n=1}^\infty A_n \in \mathcal{A}_1\cap \mathcal{A}_2 \tag{III}\]

To show these, we make use of the general rule:
\[x\in A \wedge x\in B \Leftrightarrow x\in A\cap B\]

Property (I):
\begin{align*}
	& \mathcal{A}_1\ \sigma\text{-algebra} \wedge \mathcal{A}_2\ \sigma\text{-algebra} \\
	\Rightarrow& \Omega \in \mathcal{A}_1 \wedge \Omega \in \mathcal{A}_2 \\
	\Rightarrow& \Omega \in \mathcal{A}_1 \cap \mathcal{A}_2 \\
\end{align*}

Property (II):

Let \(A \in \mathcal{A}_1 \cap \mathcal{A}_2\).
\begin{align*}
    & A \in \mathcal{A}_1\cap \mathcal{A}_2 \\
    \Rightarrow & A \in \mathcal{A}_1\ \wedge\ A \in \mathcal{A}_2 \\
    \Rightarrow & A^\complement \in \mathcal{A}_1\ \wedge\ A^\complement \in \mathcal{A}_1 & (\text{ since }\mathcal{A}_1,\mathcal{A}_2\ \sigma\text{-algebras}) \\
    \Rightarrow & A^\complement \in \mathcal{A}_1\cap\mathcal{A}_2 \\
\end{align*}
\[\therefore \forall A\in \mathcal{A}_1\cap \mathcal{A}_2: A^\complement\in\mathcal{A}_1\cap \mathcal{A}_2\]

Property (III):

Let \((A_n)_{n=1}^\infty\subseteq \mathcal{A}_1\cap \mathcal{A}_2\).

\begin{align*}
    & (A_n)_{n=1}^\infty\subseteq \mathcal{A}_1\cap \mathcal{A}_2 \\
    & (A_n)_{n=1}^\infty\subseteq \mathcal{A}_1\ \wedge\ (A_n)_{n=1}^\infty\subseteq \mathcal{A}_2 \\
    & \bigcup_{n=1}^\infty A_n \in \mathcal{A}_1\ \wedge\ \bigcup_{n=1}^\infty A_n \in \mathcal{A}_2 \\
    & \bigcup_{n=1}^\infty A_n \in \mathcal{A}_1\cap\mathcal{A}_2 \\
\end{align*}
\[\therefore \forall (A_n)_{n=1}^\infty \subseteq \mathcal{A}_1\cap\mathcal{A}_2: \bigcup_{n=1}^\infty A_n\in \mathcal{A}_1\cap \mathcal{A_2}\]

Since all three properties hold, \(\mathcal{A}_1\cap\mathcal{A}_2\) is a \(\sigma\)-algebra.
\end{proof}

\subsection{Exercise 2.6.2.5}

\(Z\sim \mathcal{N}(\mu, \sigma^2)\)
and \(X=e^Z\).

Show that the PDF of \(X\) is
\[f_X(x)=\frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{(\ln x-\mu)^2}{2\sigma^2}}\]
for \(x>0\).

\textbf{Solution:}

\begin{proof}
Let \(t \in \mathbb{R}\).
\begin{align*}
     & F_X(t) \\
    =& P(X\leq t) \\
    =& P(e^Z\leq t) \\
    =& P(\ln(e^Z)\leq \ln{t}) & (A\leq B \Rightarrow \ln{A}\leq \ln{B}\text{ since }\ln\text{ strictly increasing}) \\
    =& P(Z\leq \ln{t}) \\
    =& F_Z(\ln{t}) \\
    =& \int_{-\infty}^{\ln{t}}f_Z(\ln{z})dz \\
    =& \int_{-\infty}^{\ln{t}} \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(z-\mu)^2}{2\sigma^2}} dz \\
    =& \left[z=g(x)=\ln{x}, \frac{dz}{dx}=\frac{1}{x}\Rightarrow dz=\frac{1}{x}dx, g^{-1}(x) = e^x\right] \\
    =& \int_{g^{-1}(-\infty)}^{g^{-1}(\ln{t})} \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(g(x)-\mu)^2}{2\sigma^2}} \cdot g'(x)dx \\
    =& \int_{e^{-\infty}}^{e^{\ln{t}}} \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}} \cdot \frac{1}{x}dx \\
    =& \int_0^t \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}}dx \\
    =& \int_{-\infty}^t \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}}dx - \int_{-\infty}^0 \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}}dx \\
    =& \int_{-\infty}^t \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}}dx - P(X\leq 0) \\
    =& \int_{-\infty}^t \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}}dx & (P(X\leq 0)=0\text{ since }X=e^Z\text{ and }\forall z: e^z\geq 0)
\end{align*}
Since
\[F_X(x)=\int_{-\infty}^x f_X(u)du\]
and
\[F_X(t)=\int_{-\infty}^t \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}}dx\]
it follows that
\[f_X(x) = \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}}\]
\end{proof}

\subsection{Exercise 2.6.2.4 [draft]}

\(X=^dY\) if \(P(X\leq z)=P(Y\leq z)\).

Let \(X \sim \mathcal{N}(0, 1)\).

Show that \(X=^d-X\).

\textbf{Solution:}

\begin{align*}
	 & P(X\leq z) \\
	=& \frac{1}{\sqrt{2\pi}} \int_{-\infty}^z e^{-\frac{x^2}{2}}dx \\
	=& [y=-x, dy=-dx] \\
	=& \frac{1}{\sqrt{2\pi}} \int_{\infty}^{-z} e^{-\frac{(-y)^2}{2}}(-dy) \\
	=& P(-X\leq z) \\
\end{align*}

Hence, \(X\) symmetric distribution.

\subsection{Exercise 2.6.3.1 [draft]}

The continuous bivariate r.v. \((X,Y)\) has PDF
\[f_{X,Y}(x,y)=\casesii{1}{-y<x<y \wedge 0<y<1}{0}\]

Show that \(Cov(X,Y)=0\), but \(X\) and \(Y\) are not independent.

\textbf{Solution:}

\[Cov(X,Y) = E[XY]-E[X]E[Y]\]

\(X\) and \(Y\) independent iff
\[f_{X,Y}(x,y)=f_X(x)f_Y(y)\]
\[\Rightarrow E[XY]=\int_\mathbb{R}\int_\mathbb{R} xy f_{X,Y}(x,y)dxdy\]
\[= \int_\mathbb{R}xf_X(x)dx \int_\mathbb{R} yf_Y(y)dy = E[x]\cdot E[Y]\]
\[\Rightarrow Cov(X,Y)=0\]

Now, let's show cov is zero.

\[E[XY]=\int_\mathbb{R} \int_\mathbb{R} xy f_{X,Y}dxdy\]
\[=\int_0^1\int_{-y}^y xy1dxdy\]
\[=(symmetry, x is odd)\]
\[=\int_0^1 0=0\]

\(f_X(x)=0\) if \(|x|\geq 1\).

Let \(|x|<1\).

\[f_X(x)=\int_\mathbb{R} f_{X,Y}(x,y)dy\]

\[=\int_{|x|}^1 1dy = 1-|x|\]

\[E[X] = \int_\mathbb{R} x f_X(x)dx\]
\[= \int_0^1 x (1-|x|)dx\]
\[= (by symmetry, odd function)\]
\[= 0\]

\(x(1-|x|)\) is odd because it\'s a product of \(x\) which is odd and \(1-|x|\) is even.

Own note: The support is a triangle with corners at (0,0), (0,1), (1,1).

Now lets show that X,Y not independent.

Assume X,Y independent.

Then \(f_{X,Y}(x,y)=f_X(x)f_Y(y)\)

\[\Rightarrow f_Y(y)=\frac{f_{X,Y}(x,y)}{f_X(x)}\]

Assume \(|x|<1\) (so denominator doesnt become 0).

\[f_Y(y)=\frac{\mathds{1}_{|x|, 1}(y)}{1-|x|}\]

But tis expression is a function of \(x\), so contradiction.

And \(f_Y(y)=2y\), so it cannot be equal to the expression above.

Another way is to just compare the values of \(f_{X,Y}(x,y)\) vs \(f_X(x)f_Y(y)\).

\subsection{Exercise 2.6.3.17 [draft]}

Multivariate random variable \((X,Y)\) with joint PDF:
\[f_{X,Y}(x,y)=\casesii{\frac{2}{(1+x+y)^3}}{x>0\wedge y>0}{0}\]

Show that

a)
\[f_{X+Y}(u)=\frac{2u}{(1+u)^3}, u>0\]

b)
\[f_{X-Y}(v)=\frac{1}{2(1+v)^2},-\infty<v<\infty\]

\textbf{Solution:}

a)
We seek PDF of \(X+Y\).

\[P(X+Y\leq t)=P(X\leq t-Y)\]
\[=\int_\mathbb{R}\int_{-\infty}^{t-y} f_{X,Y}(x,y)dxdy\]

We know that \(f_{X,Y}(t)=\frac{d}{dt}P(X+Y\leq t)\)
where t is a continuity point of \(f_{X+Y}\)

\[\frac{d}{dt} P(X+Y\leq t)=\frac{d}{dt}\int_\mathbb{R}\int_{-\infty}^{t-y} f_{X,Y}(x,y)dxdy\]
\[=\int_\mathbb{R} \frac{d}{dt}\int_{\infty}^{t-y} f_{X,Y}(x,y)dxdy\]
\[=(Leibniz\ rule)\]
\[=\int_\mathbb{R} f_{X,Y}(t-y,y)dy\]

Similarly,
\[f_{X-Y}(u)=\int_\mathbb{R} f_{X,Y}(u+y,y)dy\]

a)

\[f_{X+Y}(u) = \int_\mathbb{R} f_{X,Y}(u-y,y)dy\]
\[=(u-y>0 \wedge y>0)\]
\[=\int_0^u \frac{2}{(1+u-y+y)^3}dy\]
\[=\int_0^u \frac{2}{(1+u)^3}dy\]
\[=\frac{2u}{(1+u)^3}\]

b)

Pick \(v\in \mathbb{R}\).

\[f_{X-Y}(v) = \int_\mathbb{R} f_{X,Y}(v+y,y)dy\]
\[=(v+y>0 \wedge y>0)\]
\[\int_{\max(0,-v)}^\infty \frac{2}{(1+v+2y)^3}dy\]
\[=[-\frac{1}{2} \frac{1}{(1+v+2y)^2}]_{\max(0,-v)^\infty}\]
\[\frac{1}{2(1+v+2\max(0,-v))^2}\]
\[\frac{1}{2(1+|v|)^2}\]

\section{Allan Gut problems}
\subsection{3.1, p.24}

Show that if \(X \in C(0,1)\), then so is \(1/X\).

\textbf{Solution:}

\(X\) and \(1/X\) have the same distribution if \(f_X=f_{1/X}\):

\[f_X(x;x_0,\gamma) = \frac{1}{\pi\gamma(1+\frac{(x-x_0)^2}{\gamma^2})}\]
\[f_X(x;0,1) = \frac{1}{\pi \cdot 1 \cdot (1+\frac{(x-0)^2}{1^2})}\]
\[f_X(x;0,1) = \frac{1}{\pi (x^2+1)}\]
Let \(Y=g(X)\) where \(g: \mathbb{R}\setminus\{0\}\to\mathbb{R}\setminus\{0\}\), \(g(x) = \frac{1}{x}\).

\(g\) is bijective, strictly monotone, and its inverse is:
\[g^{-1}(x) = \frac{1}{x}\]

\begin{align*}
 & f_{1/X}(y;0,1) \\
=& f_Y(y;0,1) \\
=& f_X(g^{-1}(y);0,1) \cdot |\frac{d}{dy}g^{-1}(y)| \\
=& \frac{1}{\pi (g^{-1}(y)^2+1)} \cdot |\frac{d}{dy}\frac{1}{y}| \\
=& \frac{1}{\pi ((\frac{1}{y})^2+1)} \cdot |-\frac{1}{y^2}| \\
=& \frac{1}{\pi (\frac{1}{y^2}+1)} \cdot \frac{1}{y^2} \\
=& \frac{1}{\pi (1+y^2)} \\
=& f_X(y;0,1) \\
\end{align*}

\(\therefore f_{1/X}\in C(0,1)\)

\subsection{Exercise 1.5, p.33}

Prove that if \(X\) and \(Y\) are independent
then the conditional distributions and the unconditional distributions are the same.

\textbf{Solution:}

\begin{proof}
Let \(y\in \mathbb{R}\).

Let us only consider conditioning on events with nonzero probability, so let \(x\in \text{supp}(f_X)\) so that \(X=x\) is a possible event.
\begin{align*}
     & F_{Y|X=x}(y) \\
    =& \int_{-\infty}^yf_{Y|X=x}(z)dz \\
    =& \int_{-\infty}^y\frac{f_{X,Y}(x,v)}{f_X(x)}dv & (f_X(x) \neq 0) \\
    =& \int_{-\infty}^y\frac{f_X(x)f_Y(v)}{f_X(x)}dv & (X,Y\text{ independent}) \\
    =& \int_{-\infty}^yf_Y(v)dv \\
    =& F_Y(y)
\end{align*}
\[\therefore \forall y\in \mathbb{R}:\forall x\in \text{supp}(f_X): F_{Y|X=x}(y)=F_Y(y)\]
And by symmetry, \(\forall x\in\mathbb{R}:y\in \text{supp}(f_Y):F_{X|Y=y}(x)=F_X(x)\).
\end{proof}

\subsection{1.1, p.57}

Let \(X_1,X_2 \sim U(0,1)\), iid.

Find the distribution of \(X_1+X_2\).

\textbf{Solution:}

Let \(Y=X_1+X_2\).

\begin{align*}
	 & f_Y(y) \\
	=& \int_{-\infty}^\infty f_{X_1,X_2}(x_1,y-x_1)dx_1 \\
	=& \int_{-\infty}^\infty f_{X_1}(x_1)f_{X_2}(y-x_1)dx_1 & (\text{ since } X_1,X_2\text{ iid.}) \\
	=& \int_{-\infty}^\infty \casesii{1}{0\leq x_1 \leq 1}{0}f_{X_2}(y-x_1)dx_1 \\
	=& \int_0^1 \casesii{1}{0\leq x_1 \leq 1}{0}f_{X_2}(y-x_1)dx_1 \\
	=& \int_0^1 1\cdot f_{X_2}(y-x_1)dx_1 \\
	=& \int_0^1 f_{X_2}(y-x_1)dx_1 \\
	=& \int_0^1 \casesii{1}{0\leq y-x_1\leq 1}{0}dx_1 \\
	=& \int_0^1 \casesii{1}{y-1\leq x_1\leq y}{0}dx_1 \\
	=& \int_0^1 \casesii{1}{y-1\leq x_1\leq y\wedge 0\leq x_1 \leq 1}{0}dx_1 \\
	=& \int_0^1 \casesii{1}{\max(0,y-1)\leq x_1\leq \min(1,y)}{0}dx_1 \\
	=& \casesii{\int_0^1 \casesii{1}{\max(0,y-1)\leq x_1\leq \min(1,y)}{0}dx_1}{y \leq 1}{\int_0^1 \casesii{1}{\max(0,y-1)\leq x_1\leq \min(1,y)}{0}dx_1} \\
	=& \casesii{\int_0^1 \casesii{1}{0\leq x_1\leq y}{0}dx_1}{y \leq 1}{\int_0^1 \casesii{1}{y-1\leq x_1\leq 1}{0}dx_1} \\
	=& \casesii{\int_0^y \casesii{1}{0\leq x_1\leq y}{0}dx_1}{y \leq 1}{\int_{y-1}^1 \casesii{1}{y-1\leq x_1\leq 1}{0}dx_1} \\
	=& \casesiii{\int_0^y 1dx_1}{0\leq x_1 \leq y \wedge y\leq 1}{\int_{y-1}^1 1dx_1}{y-1\leq x_1 \leq 1 \wedge \neg(y\leq 1)}{0} \\
	=& \casesiii{\int_0^y 1dx_1}{0\leq y \wedge y\leq 1}{\int_{y-1}^1 1dx_1}{y-1\leq 1 \wedge y>1}{0} \\
	=& \casesiii{\int_0^y 1dx_1}{0\leq y\leq 1}{\int_{y-1}^1 1dx_1}{1<y\leq 2}{0} \\
	=& \casesiii{\hakparen{x_1}_0^y}{0\leq y\leq 1}{\hakparen{x_1}_{y-1}^1}{1<y\leq 2}{0} \\
	=& \casesiii{y-0}{0\leq y\leq 1}{1-(y-1)}{1<y\leq 2}{0} \\
	=& \casesiii{y}{0\leq y\leq 1}{2-y}{1<y\leq 2}{0} \\
\end{align*}

\section{Old exams}
\subsection{Modelltenta Problem 1}

Let \(f : \mathbb{R} \to \mathbb{R}\) be a Borel function, and \(X\) be a
random variable. Let \(Y = f(X)\).

Prove that \(Y\) is a random variable.

\textbf{Solution:}

\begin{proof}
Let \(X:\Omega\to \mathbb{R}\).
Let \(\mathcal{A}\) be a \(\sigma\)-algebra on \(\Omega\).
Let \(\mathcal{B}\) be the Borel algebra on \(\mathbb{R}\).

\(Y\) is a random variable if \(Y\) is a measurable function w.r.t. measurable
spaces \((\Omega, \mathcal{A})\), \((\mathbb{R}, \mathcal{B})\).

\(Y\) is such a measurable function if \(\forall B\in \mathcal{B}: Y^{-1}(B)\in \mathcal{A}\).

Let
\[B\in \mathcal{B}\tag{I}\]

We need to prove that \(Y^{-1}(B)\in \mathcal{A}\).

\begin{align*}
	 & Y^{-1}(B) \\
	=& \{\omega\in \Omega: Y(\omega)\in B\} \\
	=& \{\omega\in \Omega: f(X(\omega))\in B\} \\
	=& \{\omega\in \Omega: f^{-1}(f(X(\omega)))\in f^{-1}(B)\} \\
	=& \{\omega\in \Omega: X(\omega)\in f^{-1}(B)\} \\
	=& X(f^{-1}(B)) \tag{II} \\
\end{align*}

Since \(X\) is a random variable:
\[\forall B\in \mathcal{B}: X^{-1}(B)\in \mathcal{A} \tag{III}\]

The preimage of \(f:\mathbb{R}\to\mathbb{R}\) is \(f^{-1}: \mathcal{B} \to\mathcal{B}\), so:
\[\forall B \in \mathcal{B}: f^{-1}(B)\in \mathcal{B} \tag{IV}\]

(I) and (IV) imply:
\[f^{-1}(B) \in \mathcal{B} \tag{V}\]

(V) and (III) imply:
\[X^{-1}(f^{-1}(B))\in \mathcal{A}\tag{VI}\]

(VI) and (II) imply:
\[Y^{-1}(B) \in \mathcal{A}\]

So:
\[\forall B\in \mathcal{B}:Y^{-1}(B) \in \mathcal{A}\]

Therefore, \(Y\) is a measurable function.

Therefore, \(Y\) is a random variable.
\end{proof}

\section{Misc problems}
\subsection{Prove monotonicity of expected value}
Let \(X\leq Y\).

Show that \(E[X]\leq E[Y]\).

\textbf{Solution:}

\begin{proof}
Let \(Z=Y-X\).

Then \(Z\geq 0\).
\begin{align*}
     & E[Z]\geq 0 & (\text{since }Z\geq 0 \Rightarrow E[Z]\geq 0) \\
    \Rightarrow& E[Y-X] \geq 0 \\
    \Rightarrow& E[Y]-E[X] \geq 0 & (\text{since }E\text{ linear}) \\
    \Rightarrow& E[X]\geq E[Y]
\end{align*}
\end{proof}

\subsection{Extra1 [draft]}

Let \(\mathcal{B}(\mathbb{R})\) be the Borel algebra on \(\mathbb{R}\)
defined by \(\mathcal{B}(\mathbb{R})=\sigma(\{(a,b): a<b \in \mathbb{R}\})\).

Show that \(\forall x \in \mathbb{R}:\{x\}\in \mathcal{B}(\mathbb{R})\).

\textbf{Solution:}

Let \(x\in \mathbb{R}\).

\begin{align*}
     & \{x\} \\
    =& \bigcap_{n=1}^\infty (x-\frac{1}{n}, x+\frac{1}{n}) \\
    =& \left(\bigcup_{n=1}^\infty (x-\frac{1}{n}, x+\frac{1}{n})^\complement\right)^\complement \\
    =& \left(\bigcup_{n=1}^\infty\left( (-\infty,x-\frac{1}{n})\cup (x+\frac{1}{n}      ,\infty) \right)\right)^\complement \\
\end{align*}

\(\{x\}\) is the countably infinite intersection of all intervals
\[\{(x,b):x<b\}\]

\[\cap_{n=1} A_n \]

---

\(\{x\} \in \mathcal{B}\), so complement is too.

\((-\infty,x)\cup (x,\infty)\in \mathcal{B}\)

\((-\infty,x)\in \mathcal{B}\)

Since \(\forall n: (x,x+n)\in \mathcal{B}\),
\((x,\infty)=\cup_{n=1}^\infty(x,x+n)\in \mathcal{B}\).

Similiarly
\((-\infty,x)=\cup_{n=1}^\infty (x-n,x)\in\mathcal{B}\)

Hence
\[(-\infty,x)\cup (x,\infty)\in \mathcal{B}\]
\[\{x\}\in \mathcal{B}\]

Alternatively:

\[\{x\} = \cap_{n=1}^{\infty} (x-\frac{1}{n}, x+\frac{1}{n})\]
\[= (\cup_{n=1}^{\infty} (x-\frac{1}{n}, x+\frac{1}{n})^c)^c\]

\subsection{Misc problem 1}

Prove \(P(a < X \leq b) = F_X(b)-F_X(a)\).

\textbf{Solution:}

\begin{proof}
\begin{align*}
     & P(a<X\leq b) \\
    =& 1-P(\{a<X\leq b\}^\complement) \\
    =& 1-P(X\in (a,b]^\complement) \\
    =& 1-P(X\in (-\infty,a] \cup (b,\infty)) \\
    =& 1-P(\{X\leq a\} \cup \{X>b\} \\
    =& 1-(P(X\leq a)+P(X>b)) & (\text{disjoint, }a<b) \\
    =& 1-P(X\leq a)-P(X>b) \\
    =& 1-P(X\leq a)-(1-P(X\leq b)) \\
    =& P(X\leq b)-P(X\leq a) \\
    =& F_X(b)-F_X(a)
\end{align*}
\end{proof}

\subsection{Misc problem 2 [draft]}

Random variables \(X_1,\ldots,X_n\) are independent and identically distributed.

Consider random variables \(Y_1,\ldots, Y_n\) such that
\[\forall k:Y_k=2X_k\]

Prove that \(Y_1,\ldots,Y_n\) are iid.

\textbf{Solution:}

Let \(g(t) = 2t\).

Then \(g^{-1}(t) = \frac12 t\).

And \(\forall k: Y_k=g(X_k)\).

We know that if two variables \(U\) and \(V\) are iid, then:
\[\forall t: f_U(t) = f_V(t) \tag{1}\]

Let \(i,j\in \{1,\ldots,n\}\). Let \(y\in \text{supp}(f_{Y_i})=\text{supp}(f_{Y_j})\).

\begin{align*}
     & f_{Y_i}(y) \\
    =& f_{X_i}(g^{-1}(y)) \cdot \left|\frac{d}{dy} g^{-1}(y)\right| \\
    =& f_{X_j}(g^{-1}(y)) \cdot \left|\frac{d}{dy} g^{-1}(y)\right| & (\text{apply } (I)) \\
    =& f_{Y_j}(y)
\end{align*}

\end{document}
