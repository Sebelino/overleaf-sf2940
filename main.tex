\documentclass{article}
\title{Problem set, SF2940}
\author{Ville Sebastian Olsson}
\usepackage[a4paper,margin=2em]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage[parfill]{parskip}
\usepackage{tikz}
\usepackage[breakable]{tcolorbox}
\usepackage{hyperref} % Clickable ToC
\hypersetup{colorlinks}

\usepackage{sebelino-mathlib} % Custom sty file
\setcounter{secnumdepth}{0}
\allowdisplaybreaks
\begin{document}
\maketitle
\tableofcontents

\section{Drafts}

\subsection{TK 2.6.3.20 (variant)}
Let $X_1 \sim \Gamma(r, 1)$ and $X_2 \sim \Gamma(s, 1)$ be independent.

Show that $\frac{X_1}{X_2}$ and $X_1 + X_2$ are independent.

\textbf{Solution:}

\[f_{X_1} = \frac{x^{r-1}}{\Gamma(r)}e^{-x} \mathds{1}_{[0,\infty)}(x) \]

Let
\[h(x_1, x_2) = (\frac{x_1}{x_2},x_1+x_2)\]
\[h^{-1}(x_1, x_2) = (\frac{x_1x_2}{1+x_1},\frac{x_2}{1+x_1})\]

\begin{align*}
     & f_{\frac{X_1}{X_2},X_1+X_2}(x,y) \\
    =& \frac{\partial^2}{\partial x \partial y} P(\frac{X_1}{X_2}\leq x,X_1+X_2\leq y) \\
\end{align*}

$$
\{(X1/X2, X1+X2) \in (\infty,x] \times (-\infty,y] \}
= \{(X1, X2) \in h^{-1}((\infty,x] \times (-\infty,y] \}
$$

\[J_{h^{-1}} = [v/(1+u)-uv/(1+u)^2 u/(1+u)] [-v/(1+u)^2 1/(1+u)] = v/(1+u)^2 \]

\begin{align*}
     &P(X1/X2 \leq x, X1+X2 \leq y) \\
    =& \int_{h^{-1}(interval1 x interval 2)} f_{X1,X2}(x,y)dxdy \\
    =& \int_{h^{-1}(interval1 x interval 2)} f_{X1}(x) f_{X2}(y)dxdy \\
    =& \int_{interval1 x interval 2} f_{X1}(u)f_{X_2}(v)|J_{h^{-1}}|dudv \\
\end{align*}

\[ = \int_{-\infty}^x \int_{-\infty}^y \frac{(uv/(1+u))^{r-1}}{\Gamma(r)} e^{-\frac{uv}{1+u}} e^{-\frac{v}{1+u}}\]

\[ = \int_{-\infty}^x \int_{-\infty}^y A(u) B(v) du dv\]
\[ = \int_{-\infty}^x A(u) du \int_{-\infty}^y B(v) dv\]

\[f_{X1/X2,X1+X2}(x,y) = \frac{\partial^2}{\partial x\partial y}P(\frac{X_1}{X_2}\leq x, X_1+X_2 \leq y)\]
\[ = ((x/(1+x))^{r-1} \cdot \frac{1}{(1+x)^2} \cdot \frac{1}{(1+x)^s-1}) \cdot (y^{r+s-1}e^{-y})\]
\[ = A(x) \cdot B(y)\]
Therefore independent

\subsection{TK 2.6.5.2}

Let \( X \sim \text{Exp}(1/\lambda) \) (scale parameter),
\[ L = \left\lfloor \frac{X}{m} \right\rfloor \]
\[ R = X - m \cdot L \]

Show that \(L\) and \(R\) are independent random variables.
Determine even the marginal distributions of \( L \) and \( R \).

\textbf{Solution:}

\(X\) is a continuous random variable since \(X\sim \operatorname{Exp}(\frac{1}{\lambda})\).

\(L\) is discrete since the floor function is integer-valued.

\(R\) is continuous since \(X\) is continuous and \(R=X-mL\).

\(\operatorname{supp}(f_X) = (0,\infty)\) since exponential distribution.

\(\operatorname{supp}(p_L) = \{\floor{\frac{x}{m}} : x \in (0,\infty)\} = \{0,m,2m,\ldots\} = m\mathbb{N}_0\)

\(\operatorname{supp}(f_R) = \{x-m \floor{\frac{x}{m}} : x \in (0,\infty)\} = [0,m)\)

Want to show that
\[P(L=l,\ R\leq r) = P(L=l)\cdot P(R\leq r)\]

Marginal distribution of \(L\):
\begin{align*}
    & P(L=l) \\
    & P\left(\floor{\frac{X}{m}}=l\right) \\
    & P\left(l\leq \frac{X}{m} < l+1\right) \\
    & P\left(ml\leq X < ml+m\right) \\
    & \int_{ml}^{ml+m}\lambda e^{-\lambda x}dx \\
    & -\left[e^{-\lambda x}\right]_{x=ml}^{ml+m} \\
    & -(e^{-\lambda (ml+m)} - e^{-\lambda ml}) \\
    & e^{-\lambda ml} - e^{-\lambda (ml+m)} \\
    & e^{-\lambda ml} - e^{-\lambda ml}e^{-\lambda m} \\
    & e^{-\lambda ml} (1 - e^{-\lambda m})
\end{align*}

Lemma: Show that
\[X-m\floor{\frac{X}{m}} \in [0,m) \]
Let \(L=\floor{\frac{X}{m}}\).
\begin{align*}
    & \floor{\frac{X}{m}} \leq \frac{X}{m} < \floor{\frac{X}{m}}+1 \\
    \Rightarrow& L \leq \frac{X}{m} < L+1 \\
    \Rightarrow& mL \leq X < mL+m \\
    \Rightarrow& mL-mL \leq X-mL < mL+m-mL \\
    \Rightarrow& 0 \leq X-mL < m \\
    \Rightarrow& X-mL \in [0,m) \\
    \Rightarrow& X-m\floor{\frac{X}{m}} \in [0,m)
\end{align*}

Marginal distribution of \(R\):

\begin{align*}
    & P(R\leq r) \\
    =& \sum_{l=0}^\infty P(R\leq r \cap L=l) & (\text{Law of total probability}) \\
    =& \sum_{l=0}^\infty e^{ml\lambda}(1-e^{-\lambda r}) \\
    =& (1-e^{-\lambda r})\sum_{l=0}^\infty e^{ml\lambda} \\
    =& (1-e^{-\lambda r})\sum_{l=0}^\infty (e^{m\lambda})^l \\
    =& (1-e^{-\lambda r})\frac{1}{1-e^{m\lambda}} & (\text{Geometric series: }\sum_{n=0}^\infty t^n=\frac{1}{1-t}) \\
    =& \frac{1-e^{-\lambda r}}{1-e^{m\lambda}} \\
\end{align*}

Show independence:
\begin{align*}
    & P(L=l,\ R\leq r) \\
    =& P\left(\floor{\frac{X}{m}}=l,\ X-ml\leq r\right) \\
    =& P\left(l \leq \frac{X}{m} < l+1,\ X-ml\leq r\right) \\
    =& P\left(ml \leq X < ml+m,\ X-ml\leq r\right) \\
    =& P\left(ml \leq X < ml+m,\ X\leq ml+r\right) \\
    =& P\left(ml \leq X < \min(ml+m,ml+r)\right) \\
    =& P\left(ml \leq X < ml+r\right) & (r \in [0,m)) \\
    =& F_X(ml+r) - F_X(ml) \\
    =& 1-e^{-\lambda(ml+r)} - 1 + e^{-\lambda ml} \\
    =& e^{-\lambda ml}-e^{-\lambda (ml+r)} \\
    =& e^{-\lambda ml}-e^{-\lambda ml}e^{-\lambda r} \\
    =& e^{-\lambda ml}(1-e^{-\lambda r})
\end{align*}
while:
\begin{align*}
    & P(L=l) \cdot P(R\leq r) \\
    =& p_L(l) \cdot F_R(r) \\
    =& e^{-\lambda ml} (1-e^{-m\lambda}) \cdot \frac{1-e^{-\lambda r}}{1-e^{-m\lambda}} \\
    =& e^{-\lambda ml} \cdot \frac{1-e^{-\lambda r}(1-e^{-m\lambda})}{1-e^{-m\lambda}} \\
    =& e^{-\lambda ml} (1-e^{-\lambda r}) \\
    =& P(L=l,\ R\leq r)
\end{align*}
\[\therefore L,R \text{ independent.}\]

\section{Quizzes}
\subsection{Quiz 1.2}

Let \((\Omega, \mathcal{A}, P)\) be a probability space.
Let \(A\in \mathcal{A}\) and \(B\in \mathcal{A}\)
with \(A\subseteq B\).

\textbf{a)} Is this statement correct?:
\[P(B^\complement \cap A) = P(B)-P(A)\]

\textbf{Solution:}

No. Let \(A=\varnothing\) and \(P(B)>0\).

\begin{align*}
	 & LHS \\
	=& P(B^\complement \cap A) \\
	=& P(B^\complement \cap \varnothing) \\
	=& P(\varnothing) \\
	=& 0 \\
\end{align*}
while
\begin{align*}
	  & RHS \\
	 =& P(B)-P(A) \\
	 =& P(B)-P(\varnothing) \\
	 =& P(B)-0 \\
	 =& P(B) \\
	 >& 0
\end{align*}

\textbf{b)} Is this statement correct?:
\[P(B \cap A^\complement) = P(B)-P(A)\]

\textbf{Solution:}

Yes.
\begin{proof}
\begin{align*}
	 & P(B\cap A^\complement) \\
	=& P(B\setminus A) \\
	=& P(B)-P(A) & (\text{ since }A\subseteq B) \\
\end{align*}
\end{proof}
\subsection{Quiz 1.3}

Let \((\Omega, \mathcal{A}, P)\) be a probability space.
Let \(A\in \mathcal{A}\) and \(B\in \mathcal{A}\).

Let \(P((A\cup B)^\complement)=0.5\).

Let \(P(A\cap B)=0.2\).

What is the probability that either A or B but not both will occur?

\textbf{Solution:}

\begin{align*}
	 & P((A\cup B)\setminus(A\cap B)) \\
	=& P(A\cup B)-P(A\cap B) & (\text{ since }A\cap B \subseteq A\cup B) \\
	=& P(A\cup B)-0.2 \\
	=& 1-P((A \cup B)^\complement)-0.2 \\
	=& 1-0.5-0.2 \\
	=& 0.3 \\
\end{align*}

\subsection{Quiz 1.5}

Assume
\(P(A) \geq 1-\delta\) and \(P(B) \geq 1-\delta\) for some small \(\delta\geq0\).

Show that \(P(A\cap B) \geq 1-2\delta\).

\textbf{Solution:}

\begin{align*}
	 & P(A\cup B) \\
	=& P(A)+P(B)-P(A\cap B) \\
	\Rightarrow& \\
	 & P(A\cap B) \\
	=& P(A)+P(B)-P(A\cup B) \\
	\geq& (1-\delta)+P(B)-P(A\cup B) & (\text{since }P(A) \geq 1-\delta) \\
	\geq& (1-\delta)+(1-\delta)-P(A\cup B) & (\text{since }P(B) \geq 1-\delta) \\
	\geq& (1-\delta)+(1-\delta)-1 & (\text{since }P(A\cup B) \leq 1) \\
	=& 1-2\delta
\end{align*}

\subsection{Quiz 1.6}

Let \(X\) be a random variable with continuous distribution whose CDF is given by
\[F_X(x) = \begin{cases}
	0 &\text{ if } x\leq 0 \\
	cx^2 &\text{ if } 0<x<2 \\
	1 &\text{ if } x \geq 2 \\
\end{cases}\]

\textbf{a)} Determine the constant \(c\).

\textbf{Solution:}

\(F_X(2)=1\). Since \(X\) is a continuous r.v., \(F_X\) is continuous. So:

\begin{align*}
	 & \lim_{x\uparrow 2}F_X(x)=1 \\
	\Rightarrow & c\cdot 2^2=1 \\
	\Rightarrow & c=\frac14 \\
\end{align*}

\textbf{b)} Compute \(Var(X)\).

\textbf{Solution:}

\begin{align*}
	 & f_X(x) \\
	=& \frac{d}{dx}F_X(x) \\
	=& \frac{d}{dx}\casesiii{0}{x\leq 0}{cx^2}{0<x<2}{1} \\
	=& \casesiii{0}{x\leq 0}{2cx}{0<x<2}{0} \\
	=& \casesiii{0}{x\leq 0}{\frac12x}{0<x<2}{0} \\
	=& \casesii{\frac12x}{0<x<2}{0} \\
\end{align*}
\begin{align*}
	 & E[X] \\
	=& \int_{-\infty}^\infty x f_X(x)dx \\
	=& \int_{-\infty}^\infty x \casesii{\frac12x}{0<x<2}{0}dx \\
	=& \int_{0}^2 x \frac12x dx \\
	=& \int_{0}^2 \frac12x^2 dx \\
	=& \frac16\hakparen{x^3}_0^2 dx \\
	=& \frac16(2^3-0^3) \\
	=& \frac{4}{3} \\
\end{align*}
\begin{align*}
	 & E[X^2] \\
	=& \int_{-\infty}^\infty x^2 f_X(x)dx \\
	=& \int_{-\infty}^\infty x^2 \casesii{\frac12x}{0<x<2}{0}dx \\
	=& \int_{0}^2 x^2 \frac12x dx \\
	=& \int_{0}^2 \frac12x^3 dx \\
	=& \frac18\hakparen{x^4}_0^2 \\
	=& \frac18(2^4-0^4) \\
	=& 2 \\
\end{align*}
\begin{align*}
	 & Var(X) \\
	=& E[X^2]-E[X]^2 \\
	=& 2-\left(\frac{4}{3}\right)^2 \\
	=& \frac{18}{9}-\frac{16}{9} \\
	=& \frac{2}{9} \\
\end{align*}

\subsection{Quiz 1.7}

Consider the Monty Hall problem.

Prove that the probability to win the car if you switch door is 2/3.

\textbf{Solution:}

\begin{proof}
The following events may occur, in chronological order:
\[C_k: \text{The car is placed behind door }k\]
\[S_k: \text{You select door }k\]
\[D_k: \text{You decide to open door }k\]
where \(k\in \{1,2,3\}\).

Equivalently, we want to prove that sticking to one's choice yields a probability of 1/3:
\[P(C_k|S_k \cap D_k)=\frac{1}{3}\]

The car was placed by any of the doors with equal probability, so:
\[\forall k: P(C_k)=\frac13\tag{I}\]

Since you do not know where the car is, the actual location of the car does not affect your decisions. So:
\[P(S_k\cap D_k|C_k)=P(S_k\cap D_k)\tag{II}\]
\begin{align*}
     & P(C_k|S_k\cap D_k) \\
    =& P(S_k\cap D_k|C_k)\frac{P(C_k)}{P(S_k\cap D_k)} & (\text{Bayes' theorem}) \\
    =& P(S_k\cap D_k|C_k)\frac{1/3}{P(S_k\cap D_k)} & (\text{apply (I)}) \\
    =& \frac{P(S_k\cap D_k|C_k)}{3P(S_k\cap D_k)} \\
    =& \frac{P(S_k\cap D_k)}{3P(S_k\cap D_k)} & (\text{apply (II)}) \\
    =& \frac{1}{3}
\end{align*}
Similarly, the probability of winning by switching doors is:
\begin{align*}
     & P(C_k^\complement|S_k\cap D_k^\complement) \\
    =& 1-P(C_k|S_k\cap D_k^\complement) \\
    =& 1-\frac{P(S_k\cap D_k^\complement|C_k)}{3P(S_k\cap D_k^\complement)} \\
    =& 1-\frac{1}{3} \\
    =& \frac{2}{3}
\end{align*}
\end{proof}

\section{Timo Koski problems}
\subsection{Example 1.5.1, p.26}

Let \((\Omega,\mathcal{A},P)\) be a probability space.
Let \(A\in \mathcal{A}\). Consider the indicator function of \(A\):
\[\mathds{1}_A(\omega)=\casesiie{1}{\omega\in A}{0}{\omega\notin A}\]
Show that \(\mathds{1}_A\) is a random variable.

\textbf{Solution:}
\begin{proof}
\(\mathds{1}\) is a random variable if \(\mathds{1}\) is a measurable function.

\(\mathds{1}_A\) is a measurable function w.r.t. Borel space \((\mathbb{R}, \mathcal{B})\) if
\[\forall B \in \mathcal{B}: \mathds{1}_A^{-1}(B)\in \mathcal{A}\]
Let \(B \in \mathcal{B}\).
\begin{align*}
     & \mathds{1}^{-1}_A(B) \\
    =& \{\omega\in\Omega:\mathds{1}_A(\omega)\in B\} \\
    =& \{\omega\in\Omega:\casesiie{1}{\omega\in A}{0}{\omega\notin A}\in B\} \\
    =& \{\omega\in\Omega:(1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A)\} \\
    =& \begin{cases}
    \{\omega\in\Omega:(1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A)\}&\text{ if }0\in B\wedge 1\in B\\
    \{\omega\in\Omega:(1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A)\}&\text{ if }0\in B\wedge 1\notin B\\
    \{\omega\in\Omega:(1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A)\}&\text{ if }0\notin B\wedge 1\in B\\
    \{\omega\in\Omega:(1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A)\}&\text{ if }0\notin B\wedge 1\notin B
    \end{cases} \\
    =& \begin{cases}
    \{\omega\in\Omega:((1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A))\wedge (0\in B\wedge 1\in B)\}&\text{ if }0\in B\wedge 1\in B\\
    \{\omega\in\Omega:((1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A))\wedge (0\in B\wedge 1\notin B)\}&\text{ if }0\in B\wedge 1\notin B\\
    \{\omega\in\Omega:((1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A))\wedge (0\notin B\wedge 1\in B)\}&\text{ if }0\notin B\wedge 1\in B\\
    \{\omega\in\Omega:((1\in B\wedge \omega\in A)\vee (0\in B\wedge \omega \notin A))\wedge (0\notin B\wedge 1\notin B)\}&\text{ if }0\notin B\wedge 1\notin B
    \end{cases} \\
    =& \begin{cases}
    \{\omega\in\Omega:(0\in B \wedge 1\in B\wedge \omega\in A)\vee (0\in B\wedge 1\in B\wedge \omega \notin A)\}&\text{ if }0\in B\wedge 1\in B\\
    \{\omega\in\Omega:(0\in B \wedge 1\in B\wedge1\notin B\wedge \omega\in A)\vee (0\in B\wedge 1\notin B\wedge \omega \notin A)\}&\text{ if }0\in B\wedge 1\notin B\\
    \{\omega\in\Omega:(0\notin B\wedge 1\in B\wedge \omega\in A)\vee (0\notin B \wedge 0\in B\wedge \omega \notin A)\}&\text{ if }0\notin B\wedge 1\in B\\
    \{\omega\in\Omega:(0\notin B\wedge 1\notin B \wedge 1\in B\wedge \omega\in A)\vee (0\notin B\wedge 1\notin B \wedge 0\in B\wedge \omega \notin A)\}&\text{ if }0\notin B\wedge 1\notin B
    \end{cases} \\
    =& \begin{cases}
    \{\omega\in\Omega:(\omega\in A)\vee (\omega \notin A)\}&\text{ if }0\in B\wedge 1\in B\\
    \{\omega\in\Omega:\bot\vee (\omega \notin A)\}&\text{ if }0\in B\wedge 1\notin B\\
    \{\omega\in\Omega:(\omega\in A)\vee \bot\}&\text{ if }0\notin B\wedge 1\in B\\
    \{\omega\in\Omega:\bot\vee \bot\}&\text{ if }0\notin B\wedge 1\notin B
    \end{cases} \\
    =& \begin{cases}
    \{\omega\in\Omega:\omega\in A\cup A^\complement\}&\text{ if }0\in B\wedge 1\in B\\
    \{\omega\in\Omega:\omega \notin A\}&\text{ if }0\in B\wedge 1\notin B\\
    \{\omega\in\Omega:\omega\in A\}&\text{ if }0\notin B\wedge 1\in B\\
    \{\omega\in\Omega:\bot\}&\text{ if }0\notin B\wedge 1\notin B
    \end{cases} \\
    =& \begin{cases}
    \{\omega\in\Omega:\omega\in \Omega\}&\text{ if }0\in B\wedge 1\in B\\
    \{\omega\in\Omega:\omega \in A^\complement\}&\text{ if }0\in B\wedge 1\notin B\\
    \{\omega\in\Omega:\omega\in A\}&\text{ if }0\notin B\wedge 1\in B\\
    \{\omega\in\Omega:\bot\}&\text{ if }0\notin B\wedge 1\notin B
    \end{cases} \\
    =& \begin{cases}
    \Omega &\text{ if }0\in B\wedge 1\in B\\
    A^\complement&\text{ if }0\in B\wedge 1\notin B\\
    A&\text{ if }0\notin B\wedge 1\in B\\
    \varnothing&\text{ if }0\notin B\wedge 1\notin B
    \end{cases}
\end{align*}
Since \(\mathcal{A}\) is a \(\sigma\)-algebra and \(A\in \mathcal{A}\), \(\{\Omega, A^\complement, A, \varnothing\}\subseteq \mathcal{A}\).

So \(\mathrm{img}(\mathds{1}_A^{-1}) \subseteq \mathcal{A}\).

Since \(\mathrm{img}(\mathds{1}_A^{-1})\subseteq \mathcal{A}\), \(\mathds{1}_A^{-1}(B)\in \mathcal{A}\).
\[\therefore \forall B\in \mathcal{B}: \mathds{1}_A^{-1}(B)\in \mathcal{A}\]
So \(\mathds{1}_A\) is a measurable function.

So \(\mathds{1}_A\) is a random variable.
\end{proof}

\subsection{Exercise 1.12.2.9}

Suppose \(A,B \in \mathcal{A}\).

Show that
\[\min(P(A),P(B)) \geq P(A\cap B) \geq P(A)+P(B)-1\]

\textbf{Solution:}

We first need to show that
\[\min(P(A), P(B)) \geq P(A \cap B)\]

\begin{proof}
Monotonicity theorem:
\[\forall A,B\in \mathcal{A}: (A\subseteq B \Rightarrow P(A)\leq P(B))\]

Since \(A\cap B \subseteq A\), \(P(A\cap B) \leq P(A)\).

Since \(A\cap B \subseteq B\), \(P(A\cap B) \leq P(B)\).

\begin{align*}
	 & P(A\cap B)\leq P(A) \wedge P(A\cap B)\leq P(B) \\
	\Rightarrow & P(A \cap B) \leq \min(P(A), P(B))
\end{align*}
\end{proof}

Next, we show that
\[P(A \cap B) \geq P(A)+P(B)-1\]

\begin{proof}
\begin{align*}
	 & P(A \cap B) \\
	=& P(A)+P(B)-P(A \cup B) \\
	\geq& P(A)+P(B)-1 & (\text{ since } P(B)\leq 1) \\
\end{align*}
\end{proof}

\subsection{Exercise 1.12.3.6, p.45}

Suppose that \(X\) is a random variable
such that \(P(X\geq 0) = P(\{\omega : X(\omega)\geq 0\})=1\)

Show that
\[(\forall c>0): P(X\geq c) \leq \frac{E[X]}{c}\]

\textbf{Solution 1:}

\begin{proof}
Let \(c>0\).
\begin{align*}
     & E[X] \\
    =& \int_{-\infty}^\infty x f_X(x)dx \\
    =& \int_0^\infty x f_X(x)dx & (\text{since }\mathrm{supp}(f_X)=[0,\infty)) \\
    =& \int_0^c x f_X(x)dx+\int_c^\infty x f_X(x)dx \\
    \geq& \int_c^\infty x f_X(x)dx \\
    \geq& \int_c^\infty c f_X(x)dx & (\text{since }\forall x\in [c,\infty): x\geq c) \\
    =& c\int_c^\infty f_X(x)dx \\
    =& c\left(\int_{-\infty}^\infty f_X(x)dx-\int_{-\infty}^c f_X(x)dx\right) \\
    =& c(1-F_X(c)) \\
    =& c(1-P(X\leq c)) \\
    =& cP(X\geq c) \\
    \Rightarrow & E[X] \geq c P(X\geq c) \\
    \Rightarrow & P(X\geq c) \leq \frac{E[X]}{c}
\end{align*}
\[\therefore (\forall c>0):P(X\geq c) \leq \frac{E[X]}{c}\]
\end{proof}

\textbf{Solution 2:}

\begin{proof}
Let \(c>0\).

Let \(A\in\mathcal{A}\) be an event.

Let
\[\mathds{1}_A(\omega)=\casesiie{1}{\omega\in A}{0}{\omega \notin A}\]
Then:
\[\mathds{1}_{X\geq c}(\omega)=\casesiie{1}{\omega\in \{X\geq c\}}{0}{\omega\in\{X<c\}}\]
Or just:
\[\mathds{1}_{X\geq c}=\casesiie{1}{X\geq c}{0}{X<c}\]
Law of excluded middle implies:
\[X \geq c \vee \neg(X \geq c)\]
Assume \(X\geq c\).
\begin{align*}
    & X\geq c \\
    \Rightarrow& c\leq X \\
    \Rightarrow& c\cdot 1\leq X \\
    \Rightarrow& c\cdot \casesiie{1}{X\geq c}{0}{X<c}\leq X \\
    \Rightarrow& c\cdot \mathds{1}_{X\geq c}\leq X \\
\end{align*}
Assume \(\neg(X\geq c)\).
\begin{align*}
    & \neg(X\geq c) \\
    \Rightarrow& X<c \\
    \Rightarrow& 0<X<c & (\text{since }X>0) \\
    \Rightarrow& c\cdot 0<X<c \\
    \Rightarrow& c\cdot \casesiie{1}{X\geq c}{0}{X<c} \leq X \\
    \Rightarrow& c\cdot \mathds{1}_{X\geq c}\leq X \\
\end{align*}
\begin{align*}
    & (X\geq c \Rightarrow c\cdot \mathds{1}_{X\geq c}\leq X) \wedge (\neg(X\geq c) \Rightarrow c\cdot \mathds{1}_{X\geq c}\leq X) \\
    \Rightarrow& c\cdot \mathds{1}_{X\geq c}\leq X \\
    \Rightarrow& E[c\cdot \mathds{1}_{X\geq c}]\leq E[X] & (\text{since } E\text{ monotonically non-decreasing}) \\
    \Rightarrow& c\cdot E[\mathds{1}_{X\geq c}]\leq E[X] \\
    \Rightarrow& c\sum_{x\in \{0,1\}} x\cdot p_{\mathds{1}_{X\geq c}}(x)\leq E[X] \\
    \Rightarrow& c (0\cdot p_{\mathds{1}_{X\geq c}}(0)+1\cdot p_{\mathds{1}_{X\geq c}}(1))\leq E[X] \\
    \Rightarrow& c\cdot p_{\mathds{1}_{X\geq c}}(1)\leq E[X] \\
    \Rightarrow& c\cdot P(\mathds{1}_{X\geq c}=1)\leq E[X] \\
    \Rightarrow& c\cdot P\left(\casesiie{1}{X\geq c}{0}{X<c}=1\right)\leq E[X] \\
    \Rightarrow& c\cdot P(X\geq c)\leq E[X] \\
    \Rightarrow& P(X\geq c)\leq \frac{E[X]}{c} \\
\end{align*}
\[\therefore (\forall c>0):P(X\geq c)\leq \frac{E[X]}{c}\]
\end{proof}

\subsection{Exercise 1.12.3.10}

Suppose \(\mathcal{A}_1\), \(\mathcal{A}_2\) are \(\sigma\)-algebras
of subsets of \(\Omega\).

Show that \(\mathcal{A}_1 \cap \mathcal{A}_2\) is a \(\sigma\)-algebra
of subsets of \(\Omega\).

\textbf{Solution:}
\begin{proof}
\(\mathcal{A}_1\cap\mathcal{A}_2\) is a \(\sigma\)-algebra all of the following conditions hold:
\[\Omega \in \mathcal{A}_1\cap\mathcal{A}_2 \tag{I}\]
\[\forall A\in \mathcal{A}_1\cap \mathcal{A}_2: A^\complement \in \mathcal{A}_1\cap \mathcal{A}_2 \tag{II}\]
\[\forall (A_n)_{n=1}^\infty\subseteq \mathcal{A}_1\cap \mathcal{A}_2: \bigcup_{n=1}^\infty A_n \in \mathcal{A}_1\cap \mathcal{A}_2 \tag{III}\]

To show these, we make use of the general rule:
\[x\in A \wedge x\in B \Leftrightarrow x\in A\cap B\]

Property (I):
\begin{align*}
	& \mathcal{A}_1\ \sigma\text{-algebra} \wedge \mathcal{A}_2\ \sigma\text{-algebra} \\
	\Rightarrow& \Omega \in \mathcal{A}_1 \wedge \Omega \in \mathcal{A}_2 \\
	\Rightarrow& \Omega \in \mathcal{A}_1 \cap \mathcal{A}_2 \\
\end{align*}

Property (II):

Let \(A \in \mathcal{A}_1 \cap \mathcal{A}_2\).
\begin{align*}
    & A \in \mathcal{A}_1\cap \mathcal{A}_2 \\
    \Rightarrow & A \in \mathcal{A}_1\ \wedge\ A \in \mathcal{A}_2 \\
    \Rightarrow & A^\complement \in \mathcal{A}_1\ \wedge\ A^\complement \in \mathcal{A}_1 & (\text{ since }\mathcal{A}_1,\mathcal{A}_2\ \sigma\text{-algebras}) \\
    \Rightarrow & A^\complement \in \mathcal{A}_1\cap\mathcal{A}_2 \\
\end{align*}
\[\therefore \forall A\in \mathcal{A}_1\cap \mathcal{A}_2: A^\complement\in\mathcal{A}_1\cap \mathcal{A}_2\]

Property (III):

Let \((A_n)_{n=1}^\infty\subseteq \mathcal{A}_1\cap \mathcal{A}_2\).

\begin{align*}
    & (A_n)_{n=1}^\infty\subseteq \mathcal{A}_1\cap \mathcal{A}_2 \\
    & (A_n)_{n=1}^\infty\subseteq \mathcal{A}_1\ \wedge\ (A_n)_{n=1}^\infty\subseteq \mathcal{A}_2 \\
    & \bigcup_{n=1}^\infty A_n \in \mathcal{A}_1\ \wedge\ \bigcup_{n=1}^\infty A_n \in \mathcal{A}_2 \\
    & \bigcup_{n=1}^\infty A_n \in \mathcal{A}_1\cap\mathcal{A}_2 \\
\end{align*}
\[\therefore \forall (A_n)_{n=1}^\infty \subseteq \mathcal{A}_1\cap\mathcal{A}_2: \bigcup_{n=1}^\infty A_n\in \mathcal{A}_1\cap \mathcal{A_2}\]

Since all three properties hold, \(\mathcal{A}_1\cap\mathcal{A}_2\) is a \(\sigma\)-algebra.
\end{proof}

\subsection{Exercise 2.6.2.5}

\(Z\sim \mathcal{N}(\mu, \sigma^2)\)
and \(X=e^Z\).

Show that the PDF of \(X\) is
\[f_X(x)=\frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{(\ln x-\mu)^2}{2\sigma^2}}\]
for \(x>0\).

\textbf{Solution:}

\begin{proof}
Let \(t \in \mathbb{R}\).
\begin{align*}
     & F_X(t) \\
    =& P(X\leq t) \\
    =& P(e^Z\leq t) \\
    =& P(\ln(e^Z)\leq \ln{t}) & (A\leq B \Rightarrow \ln{A}\leq \ln{B}\text{ since }\ln\text{ strictly increasing}) \\
    =& P(Z\leq \ln{t}) \\
    =& F_Z(\ln{t}) \\
    =& \int_{-\infty}^{\ln{t}}f_Z(\ln{z})dz \\
    =& \int_{-\infty}^{\ln{t}} \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(z-\mu)^2}{2\sigma^2}} dz \\
    =& \left[z=g(x)=\ln{x}, \frac{dz}{dx}=\frac{1}{x}\Rightarrow dz=\frac{1}{x}dx, g^{-1}(x) = e^x\right] \\
    =& \int_{g^{-1}(-\infty)}^{g^{-1}(\ln{t})} \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(g(x)-\mu)^2}{2\sigma^2}} \cdot g'(x)dx \\
    =& \int_{e^{-\infty}}^{e^{\ln{t}}} \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}} \cdot \frac{1}{x}dx \\
    =& \int_0^t \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}}dx \\
    =& \int_{-\infty}^t \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}}dx - \int_{-\infty}^0 \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}}dx \\
    =& \int_{-\infty}^t \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}}dx - P(X\leq 0) \\
    =& \int_{-\infty}^t \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}}dx & (P(X\leq 0)=0\text{ since }X=e^Z\text{ and }\forall z: e^z\geq 0)
\end{align*}
Since
\[F_X(x)=\int_{-\infty}^x f_X(u)du\]
and
\[F_X(t)=\int_{-\infty}^t \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}}dx\]
it follows that
\[f_X(x) = \frac{1}{x\sigma\sqrt{2\pi}} e^{-\frac{(\ln{x}-\mu)^2}{2\sigma^2}}\]
\end{proof}

\subsection{Exercise 2.6.2.4 [draft]}

\(X=^dY\) if \(P(X\leq z)=P(Y\leq z)\).

Let \(X \sim \mathcal{N}(0, 1)\).

Show that \(X=^d-X\).

\textbf{Solution:}

\begin{align*}
	 & P(X\leq z) \\
	=& \frac{1}{\sqrt{2\pi}} \int_{-\infty}^z e^{-\frac{x^2}{2}}dx \\
	=& [y=-x, dy=-dx] \\
	=& \frac{1}{\sqrt{2\pi}} \int_{\infty}^{-z} e^{-\frac{(-y)^2}{2}}(-dy) \\
	=& P(-X\leq z) \\
\end{align*}

Hence, \(X\) symmetric distribution.

\subsection{Exercise 2.6.3.1 [draft]}

The continuous bivariate r.v. \((X,Y)\) has PDF
\[f_{X,Y}(x,y)=\casesii{1}{-y<x<y \wedge 0<y<1}{0}\]

Show that \(Cov(X,Y)=0\), but \(X\) and \(Y\) are not independent.

\textbf{Solution:}

\[Cov(X,Y) = E[XY]-E[X]E[Y]\]

\(X\) and \(Y\) independent iff
\[f_{X,Y}(x,y)=f_X(x)f_Y(y)\]
\[\Rightarrow E[XY]=\int_\mathbb{R}\int_\mathbb{R} xy f_{X,Y}(x,y)dxdy\]
\[= \int_\mathbb{R}xf_X(x)dx \int_\mathbb{R} yf_Y(y)dy = E[x]\cdot E[Y]\]
\[\Rightarrow Cov(X,Y)=0\]

Now, let's show cov is zero.

\[E[XY]=\int_\mathbb{R} \int_\mathbb{R} xy f_{X,Y}dxdy\]
\[=\int_0^1\int_{-y}^y xy1dxdy\]
\[=(symmetry, x is odd)\]
\[=\int_0^1 0=0\]

\(f_X(x)=0\) if \(|x|\geq 1\).

Let \(|x|<1\).

\[f_X(x)=\int_\mathbb{R} f_{X,Y}(x,y)dy\]

\[=\int_{|x|}^1 1dy = 1-|x|\]

\[E[X] = \int_\mathbb{R} x f_X(x)dx\]
\[= \int_0^1 x (1-|x|)dx\]
\[= (by symmetry, odd function)\]
\[= 0\]

\(x(1-|x|)\) is odd because it\'s a product of \(x\) which is odd and \(1-|x|\) is even.

Own note: The support is a triangle with corners at (0,0), (0,1), (1,1).

Now lets show that X,Y not independent.

Assume X,Y independent.

Then \(f_{X,Y}(x,y)=f_X(x)f_Y(y)\)

\[\Rightarrow f_Y(y)=\frac{f_{X,Y}(x,y)}{f_X(x)}\]

Assume \(|x|<1\) (so denominator doesnt become 0).

\[f_Y(y)=\frac{\mathds{1}_{|x|, 1}(y)}{1-|x|}\]

But tis expression is a function of \(x\), so contradiction.

And \(f_Y(y)=2y\), so it cannot be equal to the expression above.

Another way is to just compare the values of \(f_{X,Y}(x,y)\) vs \(f_X(x)f_Y(y)\).

\subsection{Exercise 2.6.3.17 [draft]}

Multivariate random variable \((X,Y)\) with joint PDF:
\[f_{X,Y}(x,y)=\casesii{\frac{2}{(1+x+y)^3}}{x>0\wedge y>0}{0}\]

Show that

a)
\[f_{X+Y}(u)=\frac{2u}{(1+u)^3}, u>0\]

b)
\[f_{X-Y}(v)=\frac{1}{2(1+v)^2},-\infty<v<\infty\]

\textbf{Solution:}

a)
We seek PDF of \(X+Y\).

\[P(X+Y\leq t)=P(X\leq t-Y)\]
\[=\int_\mathbb{R}\int_{-\infty}^{t-y} f_{X,Y}(x,y)dxdy\]

We know that \(f_{X,Y}(t)=\frac{d}{dt}P(X+Y\leq t)\)
where t is a continuity point of \(f_{X+Y}\)

\[\frac{d}{dt} P(X+Y\leq t)=\frac{d}{dt}\int_\mathbb{R}\int_{-\infty}^{t-y} f_{X,Y}(x,y)dxdy\]
\[=\int_\mathbb{R} \frac{d}{dt}\int_{\infty}^{t-y} f_{X,Y}(x,y)dxdy\]
\[=(Leibniz\ rule)\]
\[=\int_\mathbb{R} f_{X,Y}(t-y,y)dy\]

Similarly,
\[f_{X-Y}(u)=\int_\mathbb{R} f_{X,Y}(u+y,y)dy\]

a)

\[f_{X+Y}(u) = \int_\mathbb{R} f_{X,Y}(u-y,y)dy\]
\[=(u-y>0 \wedge y>0)\]
\[=\int_0^u \frac{2}{(1+u-y+y)^3}dy\]
\[=\int_0^u \frac{2}{(1+u)^3}dy\]
\[=\frac{2u}{(1+u)^3}\]

b)

Pick \(v\in \mathbb{R}\).

\[f_{X-Y}(v) = \int_\mathbb{R} f_{X,Y}(v+y,y)dy\]
\[=(v+y>0 \wedge y>0)\]
\[\int_{\max(0,-v)}^\infty \frac{2}{(1+v+2y)^3}dy\]
\[=[-\frac{1}{2} \frac{1}{(1+v+2y)^2}]_{\max(0,-v)^\infty}\]
\[\frac{1}{2(1+v+2\max(0,-v))^2}\]
\[\frac{1}{2(1+|v|)^2}\]

\section{Allan Gut problems}
\subsection{3.1, p.24}

Show that if \(X \in C(0,1)\), then so is \(1/X\).

\textbf{Solution:}

\(X\) and \(1/X\) have the same distribution if \(f_X=f_{1/X}\):

\[f_X(x;x_0,\gamma) = \frac{1}{\pi\gamma(1+\frac{(x-x_0)^2}{\gamma^2})}\]
\[f_X(x;0,1) = \frac{1}{\pi \cdot 1 \cdot (1+\frac{(x-0)^2}{1^2})}\]
\[f_X(x;0,1) = \frac{1}{\pi (x^2+1)}\]
Let \(Y=g(X)\) where \(g: \mathbb{R}\setminus\{0\}\to\mathbb{R}\setminus\{0\}\), \(g(x) = \frac{1}{x}\).

\(g\) is bijective, strictly monotone, and its inverse is:
\[g^{-1}(x) = \frac{1}{x}\]

\begin{align*}
 & f_{1/X}(y;0,1) \\
=& f_Y(y;0,1) \\
=& f_X(g^{-1}(y);0,1) \cdot |\frac{d}{dy}g^{-1}(y)| \\
=& \frac{1}{\pi (g^{-1}(y)^2+1)} \cdot |\frac{d}{dy}\frac{1}{y}| \\
=& \frac{1}{\pi ((\frac{1}{y})^2+1)} \cdot |-\frac{1}{y^2}| \\
=& \frac{1}{\pi (\frac{1}{y^2}+1)} \cdot \frac{1}{y^2} \\
=& \frac{1}{\pi (1+y^2)} \\
=& f_X(y;0,1) \\
\end{align*}

\(\therefore f_{1/X}\in C(0,1)\)

\subsection{Exercise 1.5, p.33}

Prove that if \(X\) and \(Y\) are independent
then the conditional distributions and the unconditional distributions are the same.

\textbf{Solution:}

\begin{proof}
Let \(y\in \mathbb{R}\).

Let us only consider conditioning on events with nonzero probability, so let \(x\in \text{supp}(f_X)\) so that \(X=x\) is a possible event.
\begin{align*}
     & F_{Y|X=x}(y) \\
    =& \int_{-\infty}^yf_{Y|X=x}(z)dz \\
    =& \int_{-\infty}^y\frac{f_{X,Y}(x,v)}{f_X(x)}dv & (f_X(x) \neq 0) \\
    =& \int_{-\infty}^y\frac{f_X(x)f_Y(v)}{f_X(x)}dv & (X,Y\text{ independent}) \\
    =& \int_{-\infty}^yf_Y(v)dv \\
    =& F_Y(y)
\end{align*}
\[\therefore \forall y\in \mathbb{R}:\forall x\in \text{supp}(f_X): F_{Y|X=x}(y)=F_Y(y)\]
And by symmetry, \(\forall x\in\mathbb{R}:y\in \text{supp}(f_Y):F_{X|Y=y}(x)=F_X(x)\).
\end{proof}

\subsection{1.1, p.57}

Let \(X_1,X_2 \sim U(0,1)\), iid.

Find the distribution of \(X_1+X_2\).

\textbf{Solution:}

Let \(Y=X_1+X_2\).

\begin{align*}
	 & f_Y(y) \\
	=& \int_{-\infty}^\infty f_{X_1,X_2}(x_1,y-x_1)dx_1 \\
	=& \int_{-\infty}^\infty f_{X_1}(x_1)f_{X_2}(y-x_1)dx_1 & (\text{ since } X_1,X_2\text{ iid.}) \\
	=& \int_{-\infty}^\infty \casesii{1}{0\leq x_1 \leq 1}{0}f_{X_2}(y-x_1)dx_1 \\
	=& \int_0^1 \casesii{1}{0\leq x_1 \leq 1}{0}f_{X_2}(y-x_1)dx_1 \\
	=& \int_0^1 1\cdot f_{X_2}(y-x_1)dx_1 \\
	=& \int_0^1 f_{X_2}(y-x_1)dx_1 \\
	=& \int_0^1 \casesii{1}{0\leq y-x_1\leq 1}{0}dx_1 \\
	=& \int_0^1 \casesii{1}{y-1\leq x_1\leq y}{0}dx_1 \\
	=& \int_0^1 \casesii{1}{y-1\leq x_1\leq y\wedge 0\leq x_1 \leq 1}{0}dx_1 \\
	=& \int_0^1 \casesii{1}{\max(0,y-1)\leq x_1\leq \min(1,y)}{0}dx_1 \\
	=& \casesii{\int_0^1 \casesii{1}{\max(0,y-1)\leq x_1\leq \min(1,y)}{0}dx_1}{y \leq 1}{\int_0^1 \casesii{1}{\max(0,y-1)\leq x_1\leq \min(1,y)}{0}dx_1} \\
	=& \casesii{\int_0^1 \casesii{1}{0\leq x_1\leq y}{0}dx_1}{y \leq 1}{\int_0^1 \casesii{1}{y-1\leq x_1\leq 1}{0}dx_1} \\
	=& \casesii{\int_0^y \casesii{1}{0\leq x_1\leq y}{0}dx_1}{y \leq 1}{\int_{y-1}^1 \casesii{1}{y-1\leq x_1\leq 1}{0}dx_1} \\
	=& \casesiii{\int_0^y 1dx_1}{0\leq x_1 \leq y \wedge y\leq 1}{\int_{y-1}^1 1dx_1}{y-1\leq x_1 \leq 1 \wedge \neg(y\leq 1)}{0} \\
	=& \casesiii{\int_0^y 1dx_1}{0\leq y \wedge y\leq 1}{\int_{y-1}^1 1dx_1}{y-1\leq 1 \wedge y>1}{0} \\
	=& \casesiii{\int_0^y 1dx_1}{0\leq y\leq 1}{\int_{y-1}^1 1dx_1}{1<y\leq 2}{0} \\
	=& \casesiii{\hakparen{x_1}_0^y}{0\leq y\leq 1}{\hakparen{x_1}_{y-1}^1}{1<y\leq 2}{0} \\
	=& \casesiii{y-0}{0\leq y\leq 1}{1-(y-1)}{1<y\leq 2}{0} \\
	=& \casesiii{y}{0\leq y\leq 1}{2-y}{1<y\leq 2}{0} \\
\end{align*}

\subsection{Example 2.1, p.33}

A stick of length one is broken at a random point, uniformly distributed over the stick. The remaining piece is broken once more.

a) Find the expected value of the piece that now remains.

\textbf{Solution:}

Let \(X\) be the length of the piece that remains after breaking the stick once.

\(X\) is uniformly distributed: \(X\sim U(0,1)\).

Let \(Y\) be the length of the piece that remains after breaking the stick a second time.

Assuming \(X=x\), the point at which the stick will be broken the second time is denoted \(Y|X=x\), which is then a uniformly distributed random variable: \(Y\sim U(0,\frac{1}{x})\).

Its conditional expectation is:
\begin{align*}
     & E[Y|X=x] \\
    =& \int_{-\infty}^{\infty} yf_{Y|X=x}(y)dy \\
    =& \int_{-\infty}^{\infty} y\casesii{\frac{1}{x}}{0<y<x}{0}dy \\
    =& \int_0^x y\casesii{\frac{1}{x}}{0<y<x}{0}dy \\
    =& \int_0^x y\cdot\frac{1}{x}dy \\
    =& \frac{1}{x}\int_0^x ydy \\
    =& \frac{1}{x}\left[\frac12y^2\right]_0^x \\
    =& \frac{x^2}{2x} \\
    =& \frac{1}{2}x \tag{I}
\end{align*}
And the expected value of \(Y\) is:
\begin{align*}
     & E[Y] \\
    =& \int_{-\infty}^{\infty} E[Y|X=x]f_X(x)dx & (\text{law of total expectation}) \\
    =& \int_{-\infty}^{\infty} E[Y|X=x]\casesii{1}{0<x<1}{0}dx \\
    =& \int_0^1 E[Y|X=x] \cdot 1dx \\
    =& \int_0^1 E[Y|X=x]dx \\
    =& \int_0^1 \frac{1}{2}x & (\text{apply (I)}) \\
    =& \frac{1}{2}\left[\frac12x^2\right]_0^1 \\
    =& \frac{1}{4}
\end{align*}

b) Find the variance of the piece that now remains.

\begin{align*}
     & V[Y|X=x] \\
    =& E[Y^2|X=x] - (E[Y|X=x])^2 \\
    =& \underbrace{E[Y^2|X=x]}_A - (\underbrace{E[Y|X=x]}_{B})^2 \\
\end{align*}
\begin{align*}
     & A \\
    =& E[Y^2|X=x] \\
    =& \int_{-\infty}^{\infty} y^2 f_{Y|X=x}(y)dy \\
    =& \int_{-\infty}^{\infty} y^2 \casesii{\frac{1}{x}}{0<y<x}{0}dy \\
    =& \int_0^x y^2 \frac{1}{x}dy \\
    =& \frac{1}{x}\int_0^x y^2 dy \\
    =& \frac{1}{x}\left[\frac13y^3\right]_0^x \\
    =& \frac{1}{x}\cdot \frac13x^3 \\
    =& \frac13x^2
\end{align*}
\begin{align*}
     & V[Y|X=x] \\
    =& A-B^2 \\
    =& \frac{1}{3}x^2-\left(\frac{1}{2}x\right)^2 \\
    =& \frac{1}{3}x^2-\frac{1}{4}x^2 \\
    =& \frac{1}{12}x^2
\end{align*}

\subsection{Exercise 2.1}

Let \(X\), \(Y\), \(Y_1\), \(Y_2\) be random variables.

a)
Show that \(E\hakparen{c|X=x}=c\).

\textbf{Solution:}

\begin{proof}
We can interpret \(E\hakparen{c|X=x}=c\) to mean \(E\hakparen{C|X=x}=c\),
where \(C\) is a discrete random variable with PMF:
\[p_C(t) = \casesii{1}{t=c}{0}\]
Then:
\begin{align*}
     & E[c|X=x] \\
    =& E[C|X=x] \\
    =& \sum_{t\in \text{img}(C)}t\cdot p_C(t) \\
    =& \sum_{t\in \{c\}}t\cdot p_C(t) \\
    =& c\cdot p_C(c) \\
    =& c\cdot 1 \\
    =& c
\end{align*}
\end{proof}

b)
Show that
\(E\hakparen{Y_1+Y_2|X=x}=E\hakparen{Y_1|X=x}+E\hakparen{Y_2|X=x}\).

\begin{align*}
     & E\hakparen{Y_1+Y_2|X=x} \\
    =& E\hakparen{Y_1+Y_2|X=x} \\
\end{align*}

\section{Old exams}
\subsection{Modelltenta Problem 1}

Let \(f : \mathbb{R} \to \mathbb{R}\) be a Borel function, and \(X\) be a
random variable. Let \(Y = f(X)\).

Prove that \(Y\) is a random variable.

\textbf{Solution:}

\begin{proof}
Let \(X:\Omega\to \mathbb{R}\).
Let \(\mathcal{A}\) be a \(\sigma\)-algebra on \(\Omega\).
Let \(\mathcal{B}\) be the Borel algebra on \(\mathbb{R}\).

\(Y\) is a random variable if \(Y\) is a measurable function w.r.t. measurable
spaces \((\Omega, \mathcal{A})\), \((\mathbb{R}, \mathcal{B})\).

\(Y\) is such a measurable function if \(\forall B\in \mathcal{B}: Y^{-1}(B)\in \mathcal{A}\).

Let
\[B\in \mathcal{B}\tag{I}\]

We need to prove that \(Y^{-1}(B)\in \mathcal{A}\).

\begin{align*}
	 & Y^{-1}(B) \\
	=& \{\omega\in \Omega: Y(\omega)\in B\} \\
	=& \{\omega\in \Omega: f(X(\omega))\in B\} \\
	=& \{\omega\in \Omega: f^{-1}(f(X(\omega)))\in f^{-1}(B)\} \\
	=& \{\omega\in \Omega: X(\omega)\in f^{-1}(B)\} \\
	=& X(f^{-1}(B)) \tag{II} \\
\end{align*}

Since \(X\) is a random variable:
\[\forall B\in \mathcal{B}: X^{-1}(B)\in \mathcal{A} \tag{III}\]

The preimage of \(f:\mathbb{R}\to\mathbb{R}\) is \(f^{-1}: \mathcal{B} \to\mathcal{B}\), so:
\[\forall B \in \mathcal{B}: f^{-1}(B)\in \mathcal{B} \tag{IV}\]

(I) and (IV) imply:
\[f^{-1}(B) \in \mathcal{B} \tag{V}\]

(V) and (III) imply:
\[X^{-1}(f^{-1}(B))\in \mathcal{A}\tag{VI}\]

(VI) and (II) imply:
\[Y^{-1}(B) \in \mathcal{A}\]

So:
\[\forall B\in \mathcal{B}:Y^{-1}(B) \in \mathcal{A}\]

Therefore, \(Y\) is a measurable function.

Therefore, \(Y\) is a random variable.
\end{proof}

\subsection{Repetition 1 Problem 3}

\begin{sproblem}
Let \(X_1\) and \(X_2\) be two independent standard Gaussian random variables.
\end{sproblem}

\begin{ssubproblem}
b) Compute the characteristic function of the random variable \(Y = X_1\cdot X_2\).
\end{ssubproblem}

\begin{ssolution}
\(X_1,X_2 \sim \mathcal{N}(0,1)\).
\begin{align*}
	& \varphi_Y(t) \\
	=& E[e^{it Y}] \\
	=& E[e^{it X_1 X_2}] \\
	=& E[E[e^{it X_1 X_2}|X_1] & (\text{Law of total expectation})
\end{align*}
\begin{align*}
	& E[e^{it x_1 X_2}|X_1=x_1] \\
	=& E[e^{it x_1 X_2}] & (X_2,X_1\text{ independent}) \\
	=& E[e^{i (t x_1) X_2}] & (X_2,X_1\text{ independent}) \\
	=& \varphi_{X_2}(t x_1) \\
	=& \varphi_{\mathcal{N}(0,1)}(t x_1) \\
	=& e^{-\frac{1}{2}(t x_1)^2} \\
	=& e^{-\frac{1}{2}t^2 x_1^2}
\end{align*}
\begin{align*}
	& \varphi_Y(t) \\
	=& E[E[e^{it X_1 X_2}|X_1]] \\
	=& E[e^{-\frac{1}{2}t^2 X_1^2}] \\
	=& \int_{\operatorname{img}(X_1)} e^{-\frac12 t^2 x^2} f_{X_1}(x) dx \\
	=& \int_{\mathbb{R}} e^{-\frac12 t^2 x^2} \frac{1}{\sqrt{2\pi}} e^{-\frac12 x^2} dx \\
	=& \int_{\mathbb{R}} \frac{1}{\sqrt{2\pi}} e^{-\frac12 t^2 x^2}  e^{-\frac12 x^2} dx \\
	=& \int_{\mathbb{R}} \frac{1}{\sqrt{2\pi}} e^{-\frac12 t^2 x^2-\frac12 x^2} dx \\
	=& \int_{\mathbb{R}} \frac{1}{\sqrt{2\pi}} e^{-\frac12 x^2 (t^2 + 1)} dx \\
	=& \int_{\mathbb{R}} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2/(t^2 + 1)}} dx \\
	=& \int_{\mathbb{R}} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2\cdot \frac{1}{t^2 + 1}}} dx \\
	=& \int_{\mathbb{R}} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2 \sigma^2}} dx & (\text{Let }\sigma^2=\frac{1}{t^2+1}) \\
	=& \int_{\mathbb{R}} \frac{\sigma}{\sigma \sqrt{2\pi}} e^{-\frac{x^2}{2 \sigma^2}} dx \\
	=& \sigma \int_{\mathbb{R}} \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{x^2}{2 \sigma^2}} dx \\
	=& \sigma \int_{\mathbb{R}} f_Z(x)dx & (\text{Let }Z\sim \mathcal{N}(0,\sigma^2)) \\
	=& \sigma \cdot 1 \\
	=& \sqrt{\sigma^2} \\
	=& \sqrt{\frac{1}{t^2+1}} \\
	=& \frac{1}{\sqrt{t^2+1}}
\end{align*}
\end{ssolution}

\section{Misc problems}
\subsection{Independence vs. pairwise independence}
\(A\) and \(B\) are independent.

\(A\) and \(C\) are independent.

\(B\) and \(C\) are independent.

Are \(A\), \(B\), and \(C\) independent?

\textbf{Solution:}

No. Consider:
\[\Omega = \{\omega_1,\omega_2,\omega_3,\omega_4\}\]
\[\forall k: P(\{\omega_k\}) = \frac14\]
\[A = \{\omega_1,\omega_2\}\]
\[B = \{\omega_1,\omega_3\}\]
\[C = \{\omega_2,\omega_3\}\]
Then:
\[P(A \cap B) = P(\omega_1)=\frac14\]
\[P(A \cap C) = P(\omega_2)=\frac14\]
\[P(B \cap C) = P(\omega_3)=\frac14\]
but:
\[P(A\cap B \cap C) = P(\varnothing)=0\]
while:
\[P(A)\cdot P(B) \cdot P(C) = (\frac{1}{4})^3=\frac{1}{64}\neq 0\]

\subsection{Prove monotonicity of expected value}
Let \(X\leq Y\).

Show that \(E[X]\leq E[Y]\).

\textbf{Solution:}

\begin{proof}
Let \(Z=Y-X\).

Then \(Z\geq 0\).
\begin{align*}
     & E[Z]\geq 0 & (\text{since }Z\geq 0 \Rightarrow E[Z]\geq 0) \\
    \Rightarrow& E[Y-X] \geq 0 \\
    \Rightarrow& E[Y]-E[X] \geq 0 & (\text{since }E\text{ linear}) \\
    \Rightarrow& E[X]\geq E[Y]
\end{align*}
\end{proof}

\subsection{Extra1 [draft]}

Let \(\mathcal{B}(\mathbb{R})\) be the Borel algebra on \(\mathbb{R}\)
defined by \(\mathcal{B}(\mathbb{R})=\sigma(\{(a,b): a<b \in \mathbb{R}\})\).

Show that \(\forall x \in \mathbb{R}:\{x\}\in \mathcal{B}(\mathbb{R})\).

\textbf{Solution:}

Let \(x\in \mathbb{R}\).

\begin{align*}
     & \{x\} \\
    =& \bigcap_{n=1}^\infty (x-\frac{1}{n}, x+\frac{1}{n}) \\
    =& \left(\bigcup_{n=1}^\infty (x-\frac{1}{n}, x+\frac{1}{n})^\complement\right)^\complement \\
    =& \left(\bigcup_{n=1}^\infty\left( (-\infty,x-\frac{1}{n})\cup (x+\frac{1}{n}      ,\infty) \right)\right)^\complement \\
\end{align*}

\(\{x\}\) is the countably infinite intersection of all intervals
\[\{(x,b):x<b\}\]

\[\cap_{n=1} A_n \]

---

\(\{x\} \in \mathcal{B}\), so complement is too.

\((-\infty,x)\cup (x,\infty)\in \mathcal{B}\)

\((-\infty,x)\in \mathcal{B}\)

Since \(\forall n: (x,x+n)\in \mathcal{B}\),
\((x,\infty)=\cup_{n=1}^\infty(x,x+n)\in \mathcal{B}\).

Similiarly
\((-\infty,x)=\cup_{n=1}^\infty (x-n,x)\in\mathcal{B}\)

Hence
\[(-\infty,x)\cup (x,\infty)\in \mathcal{B}\]
\[\{x\}\in \mathcal{B}\]

Alternatively:

\[\{x\} = \cap_{n=1}^{\infty} (x-\frac{1}{n}, x+\frac{1}{n})\]
\[= (\cup_{n=1}^{\infty} (x-\frac{1}{n}, x+\frac{1}{n})^c)^c\]

\subsection{Misc problem 1}

Prove \(P(a < X \leq b) = F_X(b)-F_X(a)\).

\textbf{Solution:}

\begin{proof}
\begin{align*}
     & P(a<X\leq b) \\
    =& 1-P(\{a<X\leq b\}^\complement) \\
    =& 1-P(X\in (a,b]^\complement) \\
    =& 1-P(X\in (-\infty,a] \cup (b,\infty)) \\
    =& 1-P(\{X\leq a\} \cup \{X>b\} \\
    =& 1-(P(X\leq a)+P(X>b)) & (\text{disjoint, }a<b) \\
    =& 1-P(X\leq a)-P(X>b) \\
    =& 1-P(X\leq a)-(1-P(X\leq b)) \\
    =& P(X\leq b)-P(X\leq a) \\
    =& F_X(b)-F_X(a)
\end{align*}
\end{proof}

\subsection{Factorize indicator function}

\begin{sproblem}
Let
\[D = \{(x,y)\in \mathbb{R}^2 : 0 < x < y\}\]
Write \(\mathds{1}_D(x,y)\) as a product of two indicator functions,
\(\mathds{1}_A(x) \cdot \mathds{1}_B(y)\).
\end{sproblem}

\begin{ssolution}
\begin{align*}
	& \mathds{1}_D(x,y) \\
	& \mathds{1}_{\{(x,y): 0<x<y\}}(x,y) \\
	& \mathds{1}_{\{(x,y): x>0\}(x,y) \cap \{(x,y): y>x\}}(x,y) \\
	& \mathds{1}_{\{(x,y): x>0\}}(x,y) \cdot \mathds{1}_{\{(x,y): y>x\}}(x,y) & (\mathds{1}_{A \cap B} = \mathds{1}_A \cdot \mathds{1}_B) \\
	& \mathds{1}_{\{x: x>0\}}(x) \cdot \mathds{1}_{\{(x,y): y>x\}}(x,y) \\
	& \mathds{1}_{(0,\infty)}(x) \cdot \mathds{1}_{\{(x,y): y>x\}}(x,y) \\
	& \mathds{1}_{(0,\infty)}(x) \cdot \casesii{1}{(x,y) \in \{(x,y):y>x\}}{0} \\
	& \mathds{1}_{(0,\infty)}(x) \cdot \casesii{1}{y>x}{0} \\
	& \mathds{1}_{(0,\infty)}(x) \cdot \casesii{1}{y \in (x,\infty)}{0} \\
	& \mathds{1}_{(0,\infty)}(x) \cdot \mathds{1}_{(x,\infty)}(y)
\end{align*}
\end{ssolution}

\subsection{Flipping indicator functions}

\begin{sproblem}
Prove that
\[\mathds{1}_{[x,\infty)}(y) = \mathds{1}_{(-\infty,y]}(x)\]
\end{sproblem}

\begin{ssolution}
\begin{align*}
	& \mathds{1}_{[x,\infty)}(y) \\
	=& \casesii{1}{y\in [x,\infty)}{0} \\
	=& \casesii{1}{x \leq y}{0} \\
	=& \casesii{1}{x \in (-\infty,y]}{0} \\
	=& \mathds{1}_{(-\infty,y]}(x)
\end{align*}
\end{ssolution}

\subsection{Set transformation}

\begin{sproblem}
Simplify
\[T = \{(y,\frac{x}{y-x}) : 0 < x < y\}\]
\end{sproblem}

\begin{ssolution}
Find the inverse:
\begin{align*}
	& (u,v) = (y,\frac{x}{y-x}) \\
	\Rightarrow& \begin{cases}u = y \\ v = \frac{x}{y-x}\end{cases} \\
	\Rightarrow& \begin{cases}\frac{x}{y-x} = v \\ y=u\end{cases} \\
	\Rightarrow& \begin{cases}\frac{x}{u-x} = v \\ y=u\end{cases} \\
	\Rightarrow& \begin{cases}x = v(u-x) \\ y=u\end{cases} \\
	\Rightarrow& \begin{cases}x = uv-vx \\ y=u\end{cases} \\
	\Rightarrow& \begin{cases}x+vx = uv \\ y=u\end{cases} \\
	\Rightarrow& \begin{cases}x(v+1) = uv \\ y=u\end{cases} \\
	\Rightarrow& \begin{cases}x = \frac{uv}{v+1} \\ y=u\end{cases} \\
	\Rightarrow& (x,y) = (\frac{uv}{v+1}, u) \\
\end{align*}
\[\therefore T = \{(u,v) : 0 < \frac{uv}{v+1} < u\}\]
Then simplify the inequality:
\begin{align*}
	& 0 < \frac{uv}{v+1} < u \\
	\Rightarrow& \begin{cases}0 < \frac{uv}{v+1} \\ 0 < u \\ \frac{uv}{v+1} < u\end{cases} \\
	\Rightarrow& \begin{cases}0 < \frac{uv}{v+1} \\ 0 < u \\ \frac{uv}{v+1} < u \\ v+1 > 0\end{cases}
	        \vee \begin{cases}0 < \frac{uv}{v+1} \\ 0 < u \\ \frac{uv}{v+1} < u \\ v+1 < 0\end{cases} \\
	\Rightarrow& \begin{cases}0 < uv \\ 0 < u \\ uv < u(v+1) \\ v > -1\end{cases}
	        \vee \begin{cases}0 > uv \\ 0 < u \\ uv > u(v+1) \\ v < -1\end{cases} \\
	\Rightarrow& \begin{cases}uv > 0 \\ u > 0 \\ uv < uv+u \\ v > -1\end{cases}
	        \vee \begin{cases}uv < 0 \\ u > 0 \\ uv > uv+u \\ v < -1\end{cases} \\
	\Rightarrow& \begin{cases}uv > 0 \\ u > 0 \\ u > 0 \\ v > -1\end{cases}
	        \vee \begin{cases}uv < 0 \\ u > 0 \\ u < 0 \\ v < -1\end{cases} \\
	\Rightarrow& \begin{cases}uv > 0 \\ u > 0 \\ v > -1\end{cases}
	        \vee \begin{cases}u > 0 \\ u < 0\end{cases} \\
	\Rightarrow& \begin{cases}v > 0 \\ u > 0 \\ v > -1\end{cases} \vee \bot \\
	\Rightarrow& \begin{cases}u > 0 \\ v > 0\end{cases}
\end{align*}
\[\therefore T = \{(u,v) : u>0 , v>0\}\]
\end{ssolution}

\subsection{Misc problem 2 [draft]}

Random variables \(X_1,\ldots,X_n\) are independent and identically distributed.

Consider random variables \(Y_1,\ldots, Y_n\) such that
\[\forall k:Y_k=2X_k\]

Prove that \(Y_1,\ldots,Y_n\) are iid.

\textbf{Solution:}

Let \(g(t) = 2t\).

Then \(g^{-1}(t) = \frac12 t\).

And \(\forall k: Y_k=g(X_k)\).

We know that if two variables \(U\) and \(V\) are iid, then:
\[\forall t: f_U(t) = f_V(t) \tag{1}\]

Let \(i,j\in \{1,\ldots,n\}\). Let \(y\in \text{supp}(f_{Y_i})=\text{supp}(f_{Y_j})\).

\begin{align*}
     & f_{Y_i}(y) \\
    =& f_{X_i}(g^{-1}(y)) \cdot \left|\frac{d}{dy} g^{-1}(y)\right| \\
    =& f_{X_j}(g^{-1}(y)) \cdot \left|\frac{d}{dy} g^{-1}(y)\right| & (\text{apply } (I)) \\
    =& f_{Y_j}(y)
\end{align*}

\end{document}
